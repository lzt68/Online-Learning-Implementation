{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d887b295-d326-41d9-b51b-9f1dd6eeb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Sticky_TaS_fast, Sticky_TaS\n",
    "from env import Environment_Gaussian\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9209fd34-2b68-4746-b6f3-1b5db6230d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "For algorithm Sticky_TaS_fast, \n",
      "mean stop time is 194556.0\n",
      "correctness rate is 1.0\n",
      "execution time is 15.715598344802856\n"
     ]
    }
   ],
   "source": [
    "K = 1000\n",
    "xi = 0.5\n",
    "Delta = 0.01\n",
    "rlist = np.ones(K) * xi\n",
    "rlist[1:K] = xi + Delta\n",
    "rlist[0] = 1.0\n",
    "\n",
    "delta = 0.01\n",
    "n_exp = 1\n",
    "\n",
    "# for alg_class in [Sticky_TaS_fast, Sticky_TaS]:\n",
    "stop_time_ = np.zeros(n_exp)\n",
    "output_arm_ = list()\n",
    "correctness_ = np.ones(n_exp)\n",
    "exectution_time_ = np.zeros(n_exp)\n",
    "# for exp_id in tqdm(range(n_exp)):\n",
    "for exp_id in range(n_exp):\n",
    "    rlist_temp = rlist[::-1].copy()\n",
    "    # np.random.seed(exp_id)\n",
    "    # np.random.shuffle(rlist_temp)\n",
    "    answer_set = list(np.where(rlist_temp > xi)[0] + 1)\n",
    "\n",
    "    env = Environment_Gaussian(rlist=rlist_temp, K=K, random_seed=exp_id)\n",
    "    agent = Sticky_TaS_fast(K=K, delta=delta, xi=xi)\n",
    "\n",
    "    time_start = time()\n",
    "    while not agent.stop:\n",
    "        arm = agent.action()\n",
    "        reward = env.response(arm)\n",
    "        output_arm = agent.observe(reward)\n",
    "        if output_arm is not None:\n",
    "            output_arm_.append(output_arm)\n",
    "            break\n",
    "        if agent.t % 10000 == 0:\n",
    "            print(agent.t)\n",
    "    time_end = time()\n",
    "    stop_time_[exp_id] = agent.t\n",
    "    exectution_time_[exp_id] = time_end - time_start\n",
    "    if output_arm not in answer_set:\n",
    "        correctness_[exp_id] = 0\n",
    "mean_stop_time = np.mean(stop_time_)\n",
    "mean_success = np.mean(correctness_)\n",
    "mean_execution_time = np.mean(exectution_time_)\n",
    "algname = type(agent).__name__\n",
    "print(f\"For algorithm {algname}, \")\n",
    "print(f\"mean stop time is {mean_stop_time}\")\n",
    "print(f\"correctness rate is {mean_success}\")\n",
    "print(f\"execution time is {mean_execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2d88c2-f6a1-4a21-92b0-907d2221ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m arm \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39maction()\n\u001b[0;32m     18\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mresponse(arm)\n\u001b[1;32m---> 19\u001b[0m output_arm \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mobserve(reward)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_arm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     output_arm_\u001b[38;5;241m.\u001b[39mappend(output_arm)\n",
      "File \u001b[1;32mC:\\Research\\Online-Learning-Implementation\\Degenne-Koolen-2019-(Any_Large)Pure_Exploration_with_Multiple_Correct_Answers\\Source\\agent.py:231\u001b[0m, in \u001b[0;36mSticky_TaS.observe\u001b[1;34m(self, reward)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# calculate the arm to be pulled in the next round\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpulling_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# It = self.Get_It(self.mean_reward_, self.pulling_times_)\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# it = It[0]\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# wt = self.Get_wt(self.mean_reward_, it=it)\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     wt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGet_wt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_reward_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpulling_times_)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m## C-Track\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n",
      "File \u001b[1;32mC:\\Research\\Online-Learning-Implementation\\Degenne-Koolen-2019-(Any_Large)Pure_Exploration_with_Multiple_Correct_Answers\\Source\\agent.py:291\u001b[0m, in \u001b[0;36mSticky_TaS.Get_wt\u001b[1;34m(self, hatmu, pulling)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wt\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# max_mean \\geq self.xi\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGet_it(hatmu, pulling)\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hatmu[it \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxi:\n\u001b[0;32m    293\u001b[0m         wt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK)\n",
      "File \u001b[1;32mC:\\Research\\Online-Learning-Implementation\\Degenne-Koolen-2019-(Any_Large)Pure_Exploration_with_Multiple_Correct_Answers\\Source\\agent.py:340\u001b[0m, in \u001b[0;36mSticky_TaS.Get_it\u001b[1;34m(self, hatmu, pulling)\u001b[0m\n\u001b[0;32m    333\u001b[0m         Nt \u001b[38;5;241m=\u001b[39m pulling[index_better_than_arm]\n\u001b[0;32m    335\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m    336\u001b[0m             N_arm_t \u001b[38;5;241m*\u001b[39m (x \u001b[38;5;241m-\u001b[39m hat_mu_arm_t) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    337\u001b[0m             \u001b[38;5;241m+\u001b[39m Nt \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mmaximum(mu_temp \u001b[38;5;241m-\u001b[39m x, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    338\u001b[0m         )\n\u001b[1;32m--> 340\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gss(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxi, best_emp, ft):\n\u001b[0;32m    341\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m arm\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# if the algorithm doesn't return any value\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# then best_emp_arm is the minimum arm index that's in I_t\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Research\\Online-Learning-Implementation\\Degenne-Koolen-2019-(Any_Large)Pure_Exploration_with_Multiple_Correct_Answers\\Source\\agent.py:14\u001b[0m, in \u001b[0;36mgss\u001b[1;34m(f, a, b, threshold, ftoloerance)\u001b[0m\n\u001b[0;32m     12\u001b[0m d \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m (b \u001b[38;5;241m-\u001b[39m a) \u001b[38;5;241m*\u001b[39m invphi\n\u001b[0;32m     13\u001b[0m fc \u001b[38;5;241m=\u001b[39m f(c)\n\u001b[1;32m---> 14\u001b[0m fd \u001b[38;5;241m=\u001b[39m f(d)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fc \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;129;01mor\u001b[39;00m fd \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Research\\Online-Learning-Implementation\\Degenne-Koolen-2019-(Any_Large)Pure_Exploration_with_Multiple_Correct_Answers\\Source\\agent.py:337\u001b[0m, in \u001b[0;36mSticky_TaS.Get_it.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    332\u001b[0m mu_temp \u001b[38;5;241m=\u001b[39m hatmu[index_better_than_arm]\n\u001b[0;32m    333\u001b[0m Nt \u001b[38;5;241m=\u001b[39m pulling[index_better_than_arm]\n\u001b[0;32m    335\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m    336\u001b[0m     N_arm_t \u001b[38;5;241m*\u001b[39m (x \u001b[38;5;241m-\u001b[39m hat_mu_arm_t) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;241m+\u001b[39m Nt \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mmaximum(mu_temp \u001b[38;5;241m-\u001b[39m x, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    338\u001b[0m )\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gss(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxi, best_emp, ft):\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arm\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_time_ = np.zeros(n_exp)\n",
    "output_arm_ = list()\n",
    "correctness_ = np.ones(n_exp)\n",
    "exectution_time_ = np.zeros(n_exp)\n",
    "# for exp_id in tqdm(range(n_exp)):\n",
    "for exp_id in range(n_exp):\n",
    "    rlist_temp = rlist[::-1].copy()\n",
    "    # np.random.seed(exp_id)\n",
    "    # np.random.shuffle(rlist_temp)\n",
    "    answer_set = list(np.where(rlist_temp > xi)[0] + 1)\n",
    "\n",
    "    env = Environment_Gaussian(rlist=rlist_temp, K=K, random_seed=exp_id)\n",
    "    agent = Sticky_TaS(K=K, delta=delta, xi=xi)\n",
    "\n",
    "    time_start = time()\n",
    "    while not agent.stop:\n",
    "        arm = agent.action()\n",
    "        reward = env.response(arm)\n",
    "        output_arm = agent.observe(reward)\n",
    "        if output_arm is not None:\n",
    "            output_arm_.append(output_arm)\n",
    "            break\n",
    "        if agent.t % 10000 == 0:\n",
    "            print(agent.t)\n",
    "    time_end = time()\n",
    "    stop_time_[exp_id] = agent.t\n",
    "    exectution_time_[exp_id] = time_end - time_start\n",
    "    if output_arm not in answer_set:\n",
    "        correctness_[exp_id] = 0\n",
    "mean_stop_time = np.mean(stop_time_)\n",
    "mean_success = np.mean(correctness_)\n",
    "mean_execution_time = np.mean(exectution_time_)\n",
    "algname = type(agent).__name__\n",
    "print(f\"For algorithm {algname}, \")\n",
    "print(f\"mean stop time is {mean_stop_time}\")\n",
    "print(f\"correctness rate is {mean_success}\")\n",
    "print(f\"execution time is {mean_execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7888d44-a451-4b7d-89c2-35ee9378659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "round 193519 inconsistent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m arm_sas_fast \u001b[38;5;241m=\u001b[39m agent_sas_fast\u001b[38;5;241m.\u001b[39maction()\n\u001b[0;32m     45\u001b[0m arm_sas \u001b[38;5;241m=\u001b[39m agent_sas\u001b[38;5;241m.\u001b[39maction()\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m arm_sas_fast \u001b[38;5;241m==\u001b[39m arm_sas, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_sas\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inconsistent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m agent_sas\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m==\u001b[39m agent_sas_fast\u001b[38;5;241m.\u001b[39mstop, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_sas\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inconsistent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mresponse(arm_sas_fast)\n",
      "\u001b[1;31mAssertionError\u001b[0m: round 193519 inconsistent"
     ]
    }
   ],
   "source": [
    "K = 1000\n",
    "xi = 0.5\n",
    "Delta = 0.01\n",
    "rlist = np.ones(K) * xi\n",
    "rlist[1:K] = xi + Delta\n",
    "rlist[0] = 1.0\n",
    "\n",
    "delta = 0.01\n",
    "n_exp = 1\n",
    "\n",
    "\n",
    "rlist_temp = rlist[::-1].copy()\n",
    "# np.random.seed(exp_id)\n",
    "# np.random.shuffle(rlist_temp)\n",
    "answer_set = list(np.where(rlist_temp > xi)[0] + 1)\n",
    "\n",
    "env = Environment_Gaussian(rlist=rlist_temp, K=K, random_seed=0)\n",
    "agent_sas = Sticky_TaS(K=K, delta=delta, xi=xi)\n",
    "agent_sas_fast = Sticky_TaS_fast(K=K, delta=delta, xi=xi)\n",
    "\n",
    "len_statistic = K * 10\n",
    "execution_time_fast = np.zeros(len_statistic)\n",
    "execution_time = np.zeros(len_statistic)\n",
    "\n",
    "count_round = 0\n",
    "while (not agent_sas.stop) or (not agent_sas_fast.stop):\n",
    "    # start_time_fast = time()\n",
    "    # arm_sas_fast = agent_sas_fast.action()\n",
    "    # end_time_fast = time()\n",
    "\n",
    "    # start_time = time()\n",
    "    # arm_sas = agent_sas.action()\n",
    "    # end_time = time()\n",
    "\n",
    "    # arm_it_fast = agent_sas_fast.Get_it(\n",
    "    #     agent_sas_fast.mean_reward_, agent_sas_fast.pulling_times_\n",
    "    # )\n",
    "    # arm_it = agent_sas.Get_it(agent_sas.mean_reward_, agent_sas.pulling_times_)\n",
    "\n",
    "    # assert (\n",
    "    #     arm_sas_fast == arm_sas and arm_it_fast == arm_it\n",
    "    # ), f\"round {agent_sas.t} inconsistent\"\n",
    "\n",
    "    arm_sas_fast = agent_sas_fast.action()\n",
    "    arm_sas = agent_sas.action()\n",
    "    assert arm_sas_fast == arm_sas, f\"round {agent_sas.t} inconsistent\"\n",
    "    assert agent_sas.stop == agent_sas_fast.stop, f\"round {agent_sas.t} inconsistent\"\n",
    "\n",
    "    reward = env.response(arm_sas_fast)\n",
    "\n",
    "    agent_sas.observe(reward)\n",
    "    agent_sas_fast.observe(reward)\n",
    "\n",
    "    if agent_sas.t % 10000 == 0:\n",
    "        print(agent_sas.t)\n",
    "\n",
    "    # if agent_sas_fast.t <= len_statistic:\n",
    "    #     execution_time_fast[count_round] = end_time_fast - start_time_fast\n",
    "    #     execution_time[count_round] = end_time - start_time\n",
    "    #     count_round += 1\n",
    "    # else:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a380b-7907-4985-acc6-5c4648ad0751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
