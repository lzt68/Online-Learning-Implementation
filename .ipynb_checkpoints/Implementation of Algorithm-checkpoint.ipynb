{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ferreira et.al. 2018\n",
    "Following is its notation and description\n",
    "\n",
    "|      Parameter       | description                                                  |\n",
    "| :------------------: | :----------------------------------------------------------- |\n",
    "|         $K$          | Total number of available price vectors                      |\n",
    "|         $N$          | Total number of products                                    |\n",
    "|         $M$          | Total number of kinds of resource                            |\n",
    "|         $T$          | Total number of periods                                      |\n",
    "|        $[x]$         | We define $[x]$ as a set, $[x]=\\{1,2,\\cdots, x\\}$            |\n",
    "|         $i$          | Index of products                                            |\n",
    "|         $j$          | Index of resources                                           |\n",
    "|         $t$          | Index of period                                              |\n",
    "|     $I_j,I_j(t)$     | $I_j$ is the initial inventory for each resource $j\\in [M]$, $I_j(t)$ is the inventory at the end of period $t$. $I_j(0)=I_j$ |\n",
    "|       $a_{ij}$       | When we produced one unit item $i$, it would consume $a_{ij}$ unit $j$ |\n",
    "|        $c_j$         | we define $c_j=\\frac{I_j}{t}$                                |\n",
    "|        $p_k$         | We define $\\{p_1,p_2,\\cdots,p_K\\}$ as the admissible price vectors, each $p_k$ is a $N\\times 1$ vector, specifying the price of each product, $p_k=(p_{1k},\\cdots,p_{Nk})$, where $p_{ik}$ is the price of product $i$, for $i\\in [N]$. We define $p_{\\infty}$ as a \"shut-off\" price, such that the demand for any product under this price is zero. |\n",
    "|        $P(t)$        | We denote by $P(t)=(P_1(t),\\cdots,P_N(t))$ the prices chosen by the retailer in this period, and require that $P(t)\\in \\{p_1,p_2,\\cdots,p_K,p_{\\infty}\\}$ |\n",
    "|        $D(t)$        | We denote by $D(t) = (D_1(t),\\cdots,D_N(t))$ the demand of each product at period $t$. We assume that given $P(t)=p_k$, the demand $D(t)$ is sampled from a probability distribution on $\\mathbb{R}^{N}_+$ with joint cumulative distribution function (CDF) $F (x_1,\\cdots,x_N, pk, \\theta )$. $D(t)$ is independent of the history $\\mathcal{H}_{t-1}$, given $P(t)$ |\n",
    "|       $\\theta$       | $\\theta$ is the parameter of demand distribution, takes values in the parameter space $\\Theta\\subset\\mathbb{R}^l$. The nature would sample $\\theta$ from a prior distribution at the beginning form the process. The distribution is assumed to be subexponential; |\n",
    "|  $\\mathcal{H}_{t }$  | $\\mathcal{H}_{t}=(P(1),D(1),\\cdots,P(t),D(t))$               |\n",
    "|       $\\xi(t)$       | At the beginning of each period $t\\in[T]$, the retailer observes some context $\\xi(t)$, $\\xi(t)$ belongs to some discrete set $\\mathcal{X}$. We assume $\\xi(t)$ is sampled i.i.d from a known distribution. |\n",
    " | $d_{ik}(\\xi|\\theta)$ | The mean demand of product $i\\in [N]$ under price vector $p_k$, $\\forall k\\in[K]$, given context $\\xi$ and parameter $\\theta$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 1 st algorithm\n",
    "\n",
    "We implement the 4th algorithm of Ferreira et.al. 2018\n",
    "\n",
    "<img src=\"./Figure/Algorithem_1_of_Ferreira_et_al_2018.png\" style=\"zoom:80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The most difficult task in the implementation of this algorithm is the update rule of posterior. In fact, for most of the prior distribution, it is nearly impossible to derive the explict expression of posterior distribution. Here, we would like to simplify this process.\n",
    "\n",
    "We generate K * N matrix from beta(1, 1), in other words, uniform distribution in $[0, 1]$. Each entry corresponds to a product in each pricing vector. We let $d_{k,i}$ denote the demand of $i^{th}$ product given $k^{th}$ pricing vector. We assume\n",
    "$$\n",
    "\\begin{align}\n",
    "Pr(d_{k,i} = 1) & = \\Theta_{k,i}\\\\\n",
    "Pr(d_{k,i} = 0) & = 1 - \\Theta_{k,i}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, it would be quite easy to update the posterior distribution. Assume we adopt $k^{th}$ price vector $p_k$ in period $t_1, t_2, \\cdots, t_\\tau$, \n",
    "\n",
    "For each $k\\in [K], i\\in [N]$, the poseterior density function of $\\Theta_{k,i}$ is \n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\theta_{k,i} | (p^{(t_1)}_{k},d^{(t_1)}_k),\\cdots, (p^{(t_\\tau)}_{k},d^{(t_\\tau)}_k)) \n",
    "& = \\frac{Pr(d^{(t_1)}_k,\\cdots, ,d^{(t_\\tau)}_k \\ | \\ \\theta_{k,i}) * 1}{\\int_{0}^{1}Pr(d^{(t_1)}_k,\\cdots, ,d^{(t_\\tau)}_k \\ | \\ \\theta)*1d\\theta}\\\\\n",
    "&=\\frac{\\prod_{j=1}^\\tau Pr(d^{(t_j)}_k| \\ \\theta_{k,i}) * 1}{\\int_{0}^{1}\\prod_{j=1}^\\tau Pr(d^{(t_j)}_k| \\ \\theta)*1d\\theta}\\\\\n",
    "&=\\frac{\\theta_{k,i}^{\\sum_{j=1}^\\tau d^{(t_j)}_k} (1-\\theta_{k,i})^{\\tau-\\sum_{j=1}^\\tau d^{(t_j)}_k}}{\\int_{0}^{1}\\theta^{\\sum_{j=1}^\\tau d^{(t_j)}_k} (1-\\theta)^{\\tau-\\sum_{j=1}^\\tau d^{(t_j)}_k}d\\theta}\\\\\n",
    "&=beta(\\sum_{j=1}^\\tau d^{(t_j)}_k, \\tau-\\sum_{j=1}^\\tau d^{(t_j)}_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$d^{(i)}_j$ is the demand of $j^{th}$ product in $i^{th}$ period\n",
    "\n",
    "Well, there are also tough problems. That is, there would be counter-intuitive random number. For example, **higher price lead to higher demand**. But anyway, I tried my best to find a suitable prior distribution, but I failed to do so. So I decided to just use this easier one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd # we would restore our result in csv file .\\\\Result_Ferreira et.al. 2018 Algorithm 1\\\\\n",
    "import datetime\n",
    "\n",
    "import pyscipopt\n",
    "from pyscipopt import quicksum\n",
    "\n",
    "random_seed = 12345\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate parameters\n",
    "K = np.random.randint(low = 1, high = 5) # Total number of available price vectors\n",
    "N = np.random.randint(low = 1, high = 5) # Total number of products\n",
    "M = np.random.randint(low = 1, high = 5) # Total number of kinds of resource\n",
    "T = 1000 # Total number of periods\n",
    "\n",
    "# each row represent an admissible pricing strategy\n",
    "P_list = np.float64(np.random.randint(low = 1, high = 10, size = (K, N)))\n",
    "\n",
    "# initialize inventory\n",
    "I_0 = np.float64(np.random.randint(low = 9000, high = 11000, size = M))# 如果要改变T的话，这里也要对应改变\n",
    "# resource 应该和T有线性关系\n",
    "c = I_0 / T # c is the average cost of each resource\n",
    "\n",
    "# initialize a_ij\n",
    "A = np.float64(np.random.randint(low = 5, high = 15, size = (N,M)))\n",
    "\n",
    "# initialize real parameter theta\n",
    "theta = np.random.beta(a = 1, b = 1, size = (K,N))\n",
    "\n",
    "# initialize history\n",
    "H_P = np.zeros(shape = T) # the index of pricing vector we used in each period\n",
    "H_D = np.zeros(shape = (T, N)) # the demand of products in each period  \n",
    "\n",
    "H_I = np.zeros(shape = (T + 1, M)) # avaliable inventory in each period\n",
    "H_I[0, :] = np.float64(I_0)\n",
    "\n",
    "H_bestX = np.zeros(shape = (T, K + 1)) # the best solution in each optimization\n",
    "\n",
    "H_reward = np.zeros(T) # the reward in each period\n",
    "\n",
    "# each realization of price vector, index of period,\n",
    "# corresponds to a estimate of theta\n",
    "H_alpha = np.zeros(shape = (T + 1,K,N))\n",
    "H_beta = np.zeros(shape = (T + 1,K,N))\n",
    "H_alpha[0, :, :] = 1 * np.ones(shape = (K,N))\n",
    "H_beta[0, :, :] = 1 * np.ones(shape = (K,N))\n",
    "\n",
    "# initialize the constraint value in each round\n",
    "# M kinds of resources correspond to M constraints, and one more constraint is x1 + ... + xN <=1\n",
    "H_constraint_value = np.zeros(shape = (T, M + 1))\n",
    "\n",
    "# vectorize beta sample function to accelerate\n",
    "mybeta = np.vectorize(np.random.beta)\n",
    "H_theta = np.zeros(shape = (T,K,N))\n",
    "\n",
    "# vectorize binomial sample function to accelerate\n",
    "mybinomial = np.vectorize(np.random.binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward of algorithm = 3624.000000\n"
     ]
    }
   ],
   "source": [
    "# implementation of algorithm 1\n",
    "for t in range(1, T+1):\n",
    "# for t in range(1, 2):\n",
    "    # first step, sample from posterior distribution\n",
    "    # H_alpha[t-1, :, :], H_beta[t-1, :, :] is the history data from 0 to t\n",
    "    # H_theta[t-1, :, :] is the sample theta we used in round t\n",
    "    H_theta[t-1, :, :] = mybeta(H_alpha[t-1, :, :], H_beta[t-1, :, :])\n",
    "\n",
    "    # first step, calculate the mean demand given sample theta\n",
    "    demand_mean = H_theta[t-1, :, :]\n",
    "\n",
    "    # second step, optimize a linear function\n",
    "    model = pyscipopt.Model(\"Optimization in Rount {:d}\".format(t))\n",
    "    # generate decision variable\n",
    "    x = {}\n",
    "    for xindex in range(1, K+1):\n",
    "        x[xindex] = model.addVar(vtype=\"C\", lb = 0, ub = 1, name=\"x{:d}\".format(xindex))\n",
    "\n",
    "    # second step, generate object function\n",
    "    obj_coefficient = np.sum(demand_mean *  P_list, axis = 1)# obj_coefficient[k] = $\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1}$\n",
    "    model.setObjective(quicksum(x[xindex]*obj_coefficient[xindex-1] for xindex in range(1, K+1)), \"maximize\")\n",
    "    # objective = $\\sum_{k=1}^K(\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1})x_{k}$\n",
    "\n",
    "    # second step, add constraint x_1+...+x_k<=1\n",
    "    constraint_index = {}\n",
    "    constraint_index[0] = model.addCons(quicksum(x[xindex] for xindex in range(1, K+1)) <= 1)\n",
    "\n",
    "    # second step, for each resources, we require \\sum_{k=1}^K\\sum_{i=1}^N d_{i,k}a_{i,j}x_l<=c_j\n",
    "    for jj in range(1, M+1):\n",
    "        # size(A) = [N, M], size(demand_mean) = [K, N]\n",
    "        con_coefficient = A[:, jj - 1].dot(np.transpose(demand_mean))# con_coefficient[k] = $\\sum_{i=1}^N a_{i,j}d_{i,k+1}$\n",
    "        constraint_index[jj] = model.addCons(quicksum(x[xindex] * con_coefficient[xindex - 1] for xindex in range(1, K+1)) <= c[jj - 1])\n",
    "\n",
    "    # second step, optimize the problem\n",
    "    model.optimize()\n",
    "    bestx = np.zeros(K+1) # p_{K+1} would force the demand be zero\n",
    "    for xindex in range(1,K+1):\n",
    "        bestx[xindex - 1] = model.getVal(x[xindex])\n",
    "    bestx[K] = 1 - np.sum(bestx[0:K])\n",
    "    eliminate_error = lambda x: 0 if np.abs(x) < 1e-10 else x #there would be numerical error in the best solution\n",
    "    bestx = np.array([eliminate_error(x) for x in bestx])\n",
    "    bestx = bestx / np.sum(bestx)\n",
    "    \n",
    "    # third step, offer price\n",
    "    price_offered_index = np.random.choice(np.arange(1, K + 2), p = bestx)\n",
    "\n",
    "    # fourth step, update estimate of parameter\n",
    "    H_P[t - 1] = price_offered_index # record the index of offered price\n",
    "    \n",
    "    # fourth step, record the constraint value in optimization\n",
    "    H_constraint_value[t - 1, 0] = np.sum(bestx[0:K])\n",
    "    for jj in range(1, M+1):\n",
    "        con_coefficient = np.array(list(model.getValsLinear(constraint_index[jj]).values()))\n",
    "        H_constraint_value[t - 1, jj] = np.sum(bestx[0:K] * con_coefficient)\n",
    "\n",
    "    # fourth step, record the optimal solution in this round\n",
    "    H_bestX[t - 1, :] = bestx\n",
    "\n",
    "    # fourth step, record the realization of demand\n",
    "    if price_offered_index < K + 1:\n",
    "        H_D[t - 1, :] = mybinomial(np.ones(N), theta[price_offered_index - 1, :]) # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_D[t - 1, :] = np.zeros(N)\n",
    "\n",
    "    # fourth step, record the reward in this period\n",
    "    if price_offered_index < K + 1:\n",
    "        H_reward[t - 1] = P_list[price_offered_index - 1, :].dot(H_D[t - 1, :])  # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_reward[t - 1] = 0\n",
    "\n",
    "    # fourth step, record the remain inventory, size(A) = [N, M]\n",
    "    H_I[t] = H_I[t - 1] - np.transpose(A).dot(H_D[t - 1, :])\n",
    "    if not all(H_I[t] >= 0):\n",
    "        break\n",
    "\n",
    "    # fourth step, record the new estimate of alpha and beta\n",
    "    if price_offered_index < K + 1:\n",
    "        # if demand = 1, then alpha plus 1; if demand = 0, then alpha remain unchanged\n",
    "        H_alpha[t, :, :] = H_alpha[t - 1, :, :]\n",
    "        H_alpha[t, price_offered_index - 1, :] = H_alpha[t, price_offered_index - 1, :] + H_D[t - 1, :]\n",
    "\n",
    "        # if demand = 1, then beta remained unchanged; if demand = 0, then beta plus 1\n",
    "        H_beta[t, :, :] = H_beta[t - 1, :, :]\n",
    "        H_beta[t, price_offered_index - 1, :] = H_beta[t - 1, price_offered_index - 1, :] + np.ones(N) - H_D[t - 1, :]\n",
    "    else: # the demand must be zero, then all the estimate remain unchanged\n",
    "        H_alpha[t, :, :] = H_alpha[t - 1, :, :]\n",
    "        H_beta[t, :, :] = H_beta[t - 1, :, :]\n",
    "print(\"Total reward of algorithm = {:f}\".format(np.sum(H_reward)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "t, M, N, K\n",
    "constraint_index \n",
    "model.getValsLinear(constraint_index [1]), \n",
    "model.getValsLinear(constraint_index [2])\n",
    "x\n",
    "model.getVal(x[1]), model.getVal(x[2]), model.getVal(x[3])\n",
    "bestx\n",
    "0.530018723612461 * 10.376003106593089, 0.530018723612461 * 17.74088269167503, \n",
    "c\n",
    "model.getValsLinear(constraint_index [1])\n",
    "model.getValsLinear(constraint_index [2])\n",
    "model.getConss()\n",
    "model.getNConss()\n",
    "model.getValsLinear(model.getConss()[0])\n",
    "model.getValsLinear(model.getConss()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.393, 10.339])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate the csv data file and restore it into csv file\n",
    "from datetime import datetime\n",
    "moment = str(datetime.now()).replace(':', '-').replace(' ', '_')[:-7]\n",
    "# the value of moment:  '2021-10-21_11-39-24', we used this varibale to name our files\n",
    "\n",
    "# we firstly input the parameter into txt file\n",
    "prameter_filename = moment + \"_K-{:d}_N-{:d}_M-{:d}_T-{:d}_randomseed-{:d}_ParameterList.txt\".format(K, N, M, T, random_seed)\n",
    "with open(\".\\\\Result_Ferreira et.al. 2018 Algorithm 1\\\\\" + prameter_filename, 'w') as f:\n",
    "    f.write(\"K = {:d}\\n\".format(K))\n",
    "    f.write(\"N = {:d}\\n\".format(N))\n",
    "    f.write(\"M = {:d}\\n\".format(M))\n",
    "    f.write(\"randomseed = {:d}\\n\".format(random_seed))\n",
    "    \n",
    "    f.write(\"-----Admissible Prive Vector------\\n\")\n",
    "    for kindex in range(K):\n",
    "        f.write(\"Price vector {:d} : {:s}\\n\".format(kindex, str(P_list[kindex, :])))\n",
    "                \n",
    "    f.write(\"-----Initial Inventory------\\n\")\n",
    "    f.write(\"Initial Inventory: {:s}\\n\".format(str(I_0)))\n",
    "                \n",
    "    f.write(\"-----Resource Consumption------\\n\")\n",
    "    for nindex in range(N):\n",
    "        f.write(\"Cost of resource of product {:d}: {:s}\\n\".format(nindex + 1, str(A[nindex, :])))\n",
    "    \n",
    "    f.write(\"-----Resouce Constraint in each round------\\n\")\n",
    "    for nindex in range(N):\n",
    "        f.write(\"c : {:s}\\n\".format(str(c)))\n",
    "                \n",
    "    f.write(\"-----Real theta------\\n\")\n",
    "    for kindex in range(K):\n",
    "        f.write(\"Given price vector {:d}, the real theta is {:s}\\n\".format(kindex + 1, str(theta[kindex, :])))\n",
    "        \n",
    "# then we input the experiment into csc file, we use period as index\n",
    "data = pd.DataFrame({\"Pricing_index\": H_P, \"Reward\": H_reward})\n",
    "data['Period_index'] = range(1, T+1)\n",
    "# add the demand of each product\n",
    "data = pd.concat([data, pd.DataFrame(H_D, columns = [\"product_{:d}_demand\".format(nindex) for nindex in range(1,N+1)])], axis = 1) \n",
    "# add the remain invetory of each resource\n",
    "data = pd.concat([data, pd.DataFrame(H_I[0:T, :], columns = [\"resouce_{:d}\".format(mindex) for mindex in range(1,M+1)])], axis = 1)\n",
    "# add the constraint value\n",
    "data = pd.concat([data, pd.DataFrame(H_constraint_value[0:T, :], columns = [\"constraint_{:d}\".format(conindex) for conindex in range(0,M+1)])], axis = 1)\n",
    "# add the best solution in each round\n",
    "data = pd.concat([data, \n",
    "                  pd.DataFrame(H_bestX,\\\n",
    "                              columns = [\"use_price_index_{:d}\".format(kindex) for kindex in range(1,K+1)] + [\"use_infinite_price\"])],\n",
    "                  axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_alpha[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"alpha_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_beta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"beta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of theta\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_theta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"theta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "exp_filename = moment + \"_K-{:d}_N-{:d}_M-{:d}_T-{:d}_randomseed-{:d}_Experiment.csv\".format(K, N, M, T, random_seed)\n",
    "data.to_csv(\".\\\\Result_Ferreira et.al. 2018 Algorithm 1\\\\\" + exp_filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# clear the memory\n",
    "H_reward = np.zeros(T) # the reward in each period\n",
    "H_bestX = np.zeros(shape = (T, K + 1)) # the best solution in each optimization\n",
    "H_P = np.zeros(shape = T) # the index of pricing vector we used in each period\n",
    "H_D = np.zeros(shape = (T, N)) # the demand of products in each period  \n",
    "H_I = np.zeros(shape = (T + 1, M)) # avaliable inventory in each period\n",
    "H_I[0, :] = np.float64(I_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward of known distribution: 4254.000000\n"
     ]
    }
   ],
   "source": [
    "# if we replace step 1 with true theta\n",
    "for t in range(1, T+1):\n",
    "    # first step, sample from posterior distribution\n",
    "    # H_alpha[t-1, :, :], H_beta[t-1, :, :] is the history data from 0 to t\n",
    "    # H_theta[t-1, :, :] is the sample theta we used in round t\n",
    "#     H_theta[t-1, :, :] = mybeta(H_alpha[t-1, :, :], H_beta[t-1, :, :])\n",
    "\n",
    "    # first step, calculate the mean demand given sample theta\n",
    "    demand_mean = theta\n",
    "\n",
    "    # second step, optimize a linear function\n",
    "    model = pyscipopt.Model(\"Optimization in Rount {:d}\".format(t))\n",
    "    # generate decision variable\n",
    "    x = {}\n",
    "    for xindex in range(1, K+1):\n",
    "        x[xindex] = model.addVar(vtype=\"C\", lb = 0, ub = 1, name=\"x{:d}\".format(xindex))\n",
    "\n",
    "    # second step, generate object function\n",
    "    obj_coefficient = np.sum(demand_mean *  P_list, axis = 1)# obj_coefficient[k] = $\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1}$\n",
    "    model.setObjective(quicksum(x[xindex]*obj_coefficient[xindex-1] for xindex in range(1, K+1)), \"maximize\")\n",
    "    # objective = $\\sum_{k=1}^K(\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1})x_{k}$\n",
    "\n",
    "#     # second step, add constraint x_1+...+x_k<=1\n",
    "#     model.addCons(quicksum(x[xindex] for xindex in range(1, K+1)) <= 1)\n",
    "\n",
    "#     # second step, for each resources, we require \\sum_{k=1}^K\\sum_{i=1}^N d_{i,k}a_{i,j}x_l<=c_j\n",
    "#     for jj in range(1, M+1):\n",
    "#         # size(A) = [N, M], size(demand_mean) = [K, N]\n",
    "#         con_coefficient = A[:, jj - 1].dot(np.transpose(demand_mean))# con_coefficient[k] = $\\sum_{i=1}^N a_{i,j}d_{i,k+1}$\n",
    "#         model.addCons(quicksum(x[xindex] * con_coefficient[xindex - 1] for xindex in range(1, K+1)) <= c[jj - 1])\n",
    "\n",
    "#     # second step, optimize the problem\n",
    "#     model.optimize()\n",
    "#     bestx = np.zeros(K+1) # p_{K+1} would force the demand be zero\n",
    "#     for xindex in range(1,K+1):\n",
    "#         bestx[xindex - 1] = model.getVal(x[xindex])\n",
    "#     bestx[K] = 1 - np.sum(bestx[0:K])\n",
    "#     eliminate_error = lambda x: 0 if np.abs(x) < 1e-10 else x #there would be numerical error in the best solution\n",
    "#     bestx = np.array([eliminate_error(x) for x in bestx])\n",
    "#     bestx = bestx / np.sum(bestx)\n",
    "\n",
    "#     # third step, offer price\n",
    "#     price_offered_index = np.random.choice(np.arange(1, K + 2), p = bestx)\n",
    "\n",
    "    # second step, add constraint x_1+...+x_k<=1\n",
    "    constraint_index = {}\n",
    "    constraint_index[0] = model.addCons(quicksum(x[xindex] for xindex in range(1, K+1)) <= 1)\n",
    "\n",
    "    # second step, for each resources, we require \\sum_{k=1}^K\\sum_{i=1}^N d_{i,k}a_{i,j}x_l<=c_j\n",
    "    for jj in range(1, M+1):\n",
    "        # size(A) = [N, M], size(demand_mean) = [K, N]\n",
    "        con_coefficient = A[:, jj - 1].dot(np.transpose(demand_mean))# con_coefficient[k] = $\\sum_{i=1}^N a_{i,j}d_{i,k+1}$\n",
    "        constraint_index[jj] = model.addCons(quicksum(x[xindex] * con_coefficient[xindex - 1] for xindex in range(1, K+1)) <= c[jj - 1])\n",
    "\n",
    "    # second step, optimize the problem\n",
    "    model.optimize()\n",
    "    bestx = np.zeros(K+1) # p_{K+1} would force the demand be zero\n",
    "    for xindex in range(1,K+1):\n",
    "        bestx[xindex - 1] = model.getVal(x[xindex])\n",
    "    bestx[K] = 1 - np.sum(bestx[0:K])\n",
    "    eliminate_error = lambda x: 0 if np.abs(x) < 1e-10 else x #there would be numerical error in the best solution\n",
    "    bestx = np.array([eliminate_error(x) for x in bestx])\n",
    "    bestx = bestx / np.sum(bestx)\n",
    "    \n",
    "    # third step, offer price\n",
    "    price_offered_index = np.random.choice(np.arange(1, K + 2), p = bestx)\n",
    "\n",
    "    # fourth step, update estimate of parameter\n",
    "    H_P[t - 1] = price_offered_index # record the index of offered price\n",
    "    \n",
    "    # fourth step, record the constraint value in optimization\n",
    "    H_constraint_value[t - 1, 0] = np.sum(bestx[0:K])\n",
    "    for jj in range(1, M+1):\n",
    "        con_coefficient = np.array(list(model.getValsLinear(constraint_index[jj]).values()))\n",
    "        H_constraint_value[t - 1, jj] = np.sum(bestx[0:K] * con_coefficient)\n",
    "\n",
    "    # fourth step, update estimate of parameter\n",
    "    H_P[t - 1] = price_offered_index # record the index of offered price\n",
    "\n",
    "    # fourth step, record the optimal solution in this round\n",
    "    H_bestX[t - 1, :] = bestx\n",
    "\n",
    "    # fourth step, record the realization of demand\n",
    "    if price_offered_index < K + 1:\n",
    "        H_D[t - 1, :] = mybinomial(np.ones(N), theta[price_offered_index - 1, :]) # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_D[t - 1, :] = np.zeros(N)\n",
    "\n",
    "    # fourth step, record the reward in this period\n",
    "    if price_offered_index < K + 1:\n",
    "        H_reward[t - 1] = P_list[price_offered_index - 1, :].dot(H_D[t - 1, :])  # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_reward[t - 1] = 0\n",
    "\n",
    "    # fourth step, record the remain inventory, size(A) = [N, M]\n",
    "    H_I[t, :] = H_I[t - 1, :] - np.transpose(A).dot(H_D[t - 1, :])\n",
    "    if not all(H_I[t, :] >= 0):\n",
    "        break\n",
    "\n",
    "print(\"Total reward of known distribution: {:f}\".format(np.sum(H_reward)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# then input the experiment into csc file, we use period as index\n",
    "data = pd.DataFrame({\"Pricing_index\": H_P, \"Reward\": H_reward})\n",
    "data['Period_index'] = range(1, T+1)\n",
    "# add the demand of each product\n",
    "data = pd.concat([data, pd.DataFrame(H_D, columns = [\"product_{:d}_demand\".format(nindex) for nindex in range(1,N+1)])], axis = 1) \n",
    "# add the remain invetory of each resource\n",
    "data = pd.concat([data, pd.DataFrame(H_I[0:T, :], columns = [\"resouce_{:d}\".format(mindex) for mindex in range(1,M+1)])], axis = 1) \n",
    "# add the constraint value\n",
    "data = pd.concat([data, pd.DataFrame(H_constraint_value[0:T, :], columns = [\"constraint_{:d}\".format(conindex) for conindex in range(0,M+1)])], axis = 1)\n",
    "# add the best solution in each round\n",
    "data = pd.concat([data, \n",
    "                  pd.DataFrame(H_bestX,\\\n",
    "                              columns = [\"use_price_index_{:d}\".format(kindex) for kindex in range(1,K+1)] + [\"use_infinite_price\"])],\n",
    "                  axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_alpha[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"alpha_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_beta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"beta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of theta\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_theta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"theta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "exp_filename = moment + \"_K-{:d}_N-{:d}_M-{:d}_T-{:d}_randomseed-{:d}_RealThetaExperiment.csv\".format(K, N, M, T, random_seed)\n",
    "data.to_csv(\".\\\\Result_Ferreira et.al. 2018 Algorithm 1\\\\\" + exp_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2 nd algorithm\n",
    "\n",
    "We implement the 4th algorithm of Ferreira et.al. 2018\n",
    "\n",
    "<img src=\"./Figure/Algorithem_2_of_Ferreira_et_al_2018.png\" style=\"zoom:80%\" />\n",
    "\n",
    "$c_j(t) = \\frac{I_j(t-1)}{T-t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyscipopt\n",
    "from pyscipopt import quicksum\n",
    "\n",
    "random_seed = 12345\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate parameters\n",
    "K = np.random.randint(low = 1, high = 5) # Total number of available price vectors\n",
    "N = np.random.randint(low = 1, high = 5) # Total number of products\n",
    "M = np.random.randint(low = 1, high = 5) # Total number of kinds of resource\n",
    "T = 1000 # Total number of periods\n",
    "\n",
    "# generate the demand \n",
    "\n",
    "# each row represent an admissible pricing strategy\n",
    "P_list = np.float64(np.random.randint(low = 1, high = 10, size = (K, N)))\n",
    "\n",
    "# initialize inventory\n",
    "I_0 = np.float64(np.random.randint(low = 9000, high = 11000, size = M))\n",
    "c = np.zeros(shape = (T, M)) # we would update c in each period\n",
    "\n",
    "# initialize a_ij\n",
    "A = np.float64(np.random.randint(low = 5, high = 15, size = (N,M)))\n",
    "\n",
    "# initialize real parameter theta\n",
    "theta = np.random.beta(a = 1, b = 1, size = (K,N))\n",
    "\n",
    "# initialize history\n",
    "H_P = np.zeros(shape = T) # the index of pricing vector we used in each period\n",
    "H_D = np.zeros(shape = (T, N)) # the demand of products in each period  \n",
    "\n",
    "H_I = np.zeros(shape = (T + 1, M)) # avaliable inventory in each period\n",
    "H_I[0, :] = np.float64(I_0)\n",
    "\n",
    "H_bestX = np.zeros(shape = (T, K + 1)) # the best solution in each optimization\n",
    "\n",
    "H_reward = np.zeros(T) # the reward in each period\n",
    "\n",
    "# each realization of price vector, index of period,\n",
    "# corresponds to a estimate of theta\n",
    "H_alpha = np.zeros(shape = (T + 1,K,N))\n",
    "H_beta = np.zeros(shape = (T + 1,K,N))\n",
    "H_alpha[0, :, :] = 1*np.ones(shape = (K,N))\n",
    "H_beta[0, :, :] = 1*np.ones(shape = (K,N))\n",
    "\n",
    "# initialize the constraint value in each round\n",
    "# M kinds of resources correspond to M constraints, and one more constraint is x1 + ... + xN <=1\n",
    "H_constraint_value = np.zeros(shape = (T, M + 1))\n",
    "\n",
    "# vectorize beta sample function to accelerate\n",
    "mybeta = np.vectorize(np.random.beta)\n",
    "H_theta = np.zeros(shape = (T,K,N))\n",
    "\n",
    "# vectorize binomial sample function to accelerate\n",
    "mybinomial = np.vectorize(np.random.binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward of algorithm = 4259.000000\n"
     ]
    }
   ],
   "source": [
    "# implementation of algorithm 2\n",
    "for t in range(1, T+1):\n",
    "    # first step, sample from posterior distribution\n",
    "    # H_alpha[t-1, :, :], H_beta[t-1, :, :] is the history data from 0 to t\n",
    "    # H_theta[t-1, :, :] is the sample theta we used in round t\n",
    "    H_theta[t-1, :, :] = mybeta(H_alpha[t-1, :, :], H_beta[t-1, :, :])\n",
    "\n",
    "    # first step, calculate the mean demand given sample theta\n",
    "    demand_mean = H_theta[t-1, :, :]\n",
    "\n",
    "    # second step, optimize a linear function\n",
    "    model = pyscipopt.Model(\"Optimization in Rount {:d}\".format(t))\n",
    "    # generate decision variable\n",
    "    x = {}\n",
    "    for xindex in range(1, K+1):\n",
    "        x[xindex] = model.addVar(vtype=\"C\", lb = 0, ub = 1, name=\"x{:d}\".format(xindex))\n",
    "\n",
    "    # second step, generate object function\n",
    "    obj_coefficient = np.sum(demand_mean *  P_list, axis = 1)# obj_coefficient[k] = $\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1}$\n",
    "    model.setObjective(quicksum(x[xindex]*obj_coefficient[xindex-1] for xindex in range(1, K+1)), \"maximize\")\n",
    "    # objective = $\\sum_{k=1}^K(\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1})x_{k}$\n",
    "\n",
    "#     # second step, add constraint x_1+...+x_k<=1\n",
    "#     model.addCons(quicksum(x[xindex] for xindex in range(1, K+1)) <= 1)\n",
    "\n",
    "#     # second step, for each resources, we require \\sum_{k=1}^K\\sum_{i=1}^N d_{i,k}a_{i,j}x_l<=c_j\n",
    "#     c[t - 1, :] = H_I[t - 1, :] / (T - t + 1)\n",
    "#     for jj in range(1, M+1):\n",
    "#         # size(A) = [N, M], size(demand_mean) = [K, N]\n",
    "#         con_coefficient = A[:, jj - 1].dot(np.transpose(demand_mean))# con_coefficient[k] = $\\sum_{i=1}^N a_{i,j}d_{i,k+1}$\n",
    "#         model.addCons(quicksum(x[xindex] * con_coefficient[xindex - 1] for xindex in range(1, K+1)) <= c[t - 1, jj - 1])\n",
    "\n",
    "#     # second step, optimize the problem\n",
    "#     model.optimize()\n",
    "#     bestx = np.zeros(K+1) # p_{K+1} would force the demand be zero\n",
    "#     for xindex in range(1,K+1):\n",
    "#         bestx[xindex - 1] = model.getVal(x[xindex])\n",
    "#     bestx[K] = 1 - np.sum(bestx[0:K])\n",
    "#     eliminate_error = lambda x: 0 if np.abs(x) < 1e-10 else x #there would be numerical error in the best solution\n",
    "#     bestx = np.array([eliminate_error(x) for x in bestx])\n",
    "#     bestx = bestx / np.sum(bestx)\n",
    "\n",
    "\n",
    "#     # third step, offer price\n",
    "#     price_offered_index = np.random.choice(np.arange(1, K + 2), p = bestx)\n",
    "\n",
    "    # second step, add constraint x_1+...+x_k<=1\n",
    "    constraint_index = {}\n",
    "    constraint_index[0] = model.addCons(quicksum(x[xindex] for xindex in range(1, K+1)) <= 1)\n",
    "\n",
    "    # second step, for each resources, we require \\sum_{k=1}^K\\sum_{i=1}^N d_{i,k}a_{i,j}x_l<=c_j\n",
    "    c[t - 1, :] = H_I[t - 1, :] / (T - t + 1)\n",
    "    for jj in range(1, M+1):\n",
    "        # size(A) = [N, M], size(demand_mean) = [K, N]\n",
    "        con_coefficient = A[:, jj - 1].dot(np.transpose(demand_mean))# con_coefficient[k] = $\\sum_{i=1}^N a_{i,j}d_{i,k+1}$\n",
    "        constraint_index[jj] = model.addCons(quicksum(x[xindex] * con_coefficient[xindex - 1] for xindex in range(1, K+1)) <= c[t - 1, jj - 1])\n",
    "\n",
    "    # second step, optimize the problem\n",
    "    model.optimize()\n",
    "    bestx = np.zeros(K+1) # p_{K+1} would force the demand be zero\n",
    "    for xindex in range(1,K+1):\n",
    "        bestx[xindex - 1] = model.getVal(x[xindex])\n",
    "    bestx[K] = 1 - np.sum(bestx[0:K])\n",
    "    eliminate_error = lambda x: 0 if np.abs(x) < 1e-10 else x #there would be numerical error in the best solution\n",
    "    bestx = np.array([eliminate_error(x) for x in bestx])\n",
    "    bestx = bestx / np.sum(bestx)\n",
    "    \n",
    "    # third step, offer price\n",
    "    price_offered_index = np.random.choice(np.arange(1, K + 2), p = bestx)\n",
    "\n",
    "    # fourth step, update estimate of parameter\n",
    "    H_P[t - 1] = price_offered_index # record the index of offered price\n",
    "    \n",
    "    # fourth step, record the constraint value in optimization\n",
    "    H_constraint_value[t - 1, 0] = np.sum(bestx[0:K])\n",
    "    for jj in range(1, M+1):\n",
    "        con_coefficient = np.array(list(model.getValsLinear(constraint_index[jj]).values()))\n",
    "        H_constraint_value[t - 1, jj] = np.sum(bestx[0:K] * con_coefficient)\n",
    "\n",
    "    # fourth step, update estimate of parameter\n",
    "    H_P[t - 1] = price_offered_index # record the index of offered price\n",
    "\n",
    "    # fourth step, record the optimal solution in this round\n",
    "    H_bestX[t - 1, :] = bestx\n",
    "\n",
    "    # fourth step, record the realization of demand\n",
    "    if price_offered_index < K + 1:\n",
    "        H_D[t - 1, :] = mybinomial(np.ones(N), theta[price_offered_index - 1, :]) # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_D[t - 1, :] = np.zeros(N)\n",
    "\n",
    "    # fourth step, record the reward in this period\n",
    "    if price_offered_index < K + 1:\n",
    "        H_reward[t - 1] = P_list[price_offered_index - 1, :].dot(H_D[t - 1, :])  # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_reward[t - 1] = 0\n",
    "\n",
    "    # fourth step, record the remain inventory, size(A) = [N, M]\n",
    "    H_I[t] = H_I[t - 1] - np.transpose(A).dot(H_D[t - 1, :])\n",
    "    if not all(H_I[t] >= 0):\n",
    "        break\n",
    "\n",
    "    # fourth step, record the new estimate of alpha and beta\n",
    "    if price_offered_index < K + 1:\n",
    "        # if demand = 1, then alpha plus 1; if demand = 0, then alpha remain unchanged\n",
    "        H_alpha[t, :, :] = H_alpha[t - 1, :, :]\n",
    "        H_alpha[t, price_offered_index - 1, :] = H_alpha[t, price_offered_index - 1, :] + H_D[t - 1, :]\n",
    "\n",
    "        # if demand = 1, then beta remained unchanged; if demand = 0, then beta plus 1\n",
    "        H_beta[t, :, :] = H_beta[t - 1, :, :]\n",
    "        H_beta[t, price_offered_index - 1, :] = H_beta[t - 1, price_offered_index - 1, :] + np.ones(N) - H_D[t - 1, :]\n",
    "    else: # the demand must be zero, then all the estimate remain unchanged\n",
    "        H_alpha[t, :, :] = H_alpha[t - 1, :, :]\n",
    "        H_beta[t, :, :] = H_beta[t - 1, :, :]\n",
    "print(\"Total reward of algorithm = {:f}\".format(np.sum(H_reward)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate the csv data file and restore it into csv file\n",
    "from datetime import datetime\n",
    "moment = str(datetime.now()).replace(':', '-').replace(' ', '_')[:-7]\n",
    "# the value of moment:  '2021-10-21_11-39-24', we used this varibale to name our files\n",
    "\n",
    "# we firstly input the parameter into txt file\n",
    "prameter_filename = moment + \"_K-{:d}_N-{:d}_M-{:d}_T-{:d}_randomseed-{:d}_ParameterList.txt\".format(K, N, M, T, random_seed)\n",
    "with open(\".\\\\Result_Ferreira et.al. 2018 Algorithm 2\\\\\" + prameter_filename, 'w') as f:\n",
    "    f.write(\"K = {:d}\\n\".format(K))\n",
    "    f.write(\"N = {:d}\\n\".format(N))\n",
    "    f.write(\"M = {:d}\\n\".format(M))\n",
    "    f.write(\"randomseed = {:d}\\n\".format(random_seed))\n",
    "    \n",
    "    f.write(\"-----Admissible Prive Vector------\\n\")\n",
    "    for kindex in range(K):\n",
    "        f.write(\"Price vector {:d} : {:s}\\n\".format(kindex, str(P_list[kindex, :])))\n",
    "                \n",
    "    f.write(\"-----Initial Inventory------\\n\")\n",
    "    f.write(\"Initial Inventory: {:s}\\n\".format(str(I_0)))\n",
    "                \n",
    "    f.write(\"-----Resource Consumption------\\n\")\n",
    "    for nindex in range(N):\n",
    "        f.write(\"Cost of resource of product {:d}: {:s}\\n\".format(nindex + 1, str(A[nindex, :])))\n",
    "                \n",
    "    f.write(\"-----Real theta------\\n\")\n",
    "    for kindex in range(K):\n",
    "        f.write(\"Given price vector {:d}, the real theta is {:s}\\n\".format(kindex + 1, str(theta[kindex, :])))\n",
    "        \n",
    "# then we input the experiment into csc file, we use period as index\n",
    "data = pd.DataFrame({\"Pricing_index\": H_P, \"Reward\": H_reward})\n",
    "data['Period_index'] = range(1, T+1)\n",
    "# add the demand of each product\n",
    "data = pd.concat([data, pd.DataFrame(H_D, columns = [\"product_{:d}_demand\".format(nindex) for nindex in range(1,N+1)])], axis = 1) \n",
    "# add the remain invetory of each resource\n",
    "data = pd.concat([data, pd.DataFrame(H_I[0:T, :], columns = [\"resouce_{:d}\".format(mindex) for mindex in range(1,M+1)])], axis = 1) \n",
    "# add the constraint value\n",
    "data = pd.concat([data, pd.DataFrame(H_constraint_value[0:T, :], columns = [\"constraint_{:d}\".format(conindex) for conindex in range(0,M+1)])], axis = 1)\n",
    "# add the resource limits\n",
    "data = pd.concat([data, pd.DataFrame(c[0:T, :], columns = [\"c_{:d}\".format(conindex) for conindex in range(1,M+1)])], axis = 1)\n",
    "# add the best solution in each round\n",
    "data = pd.concat([data, \n",
    "                  pd.DataFrame(H_bestX,\\\n",
    "                              columns = [\"use_price_index_{:d}\".format(kindex) for kindex in range(1,K+1)] + [\"use_infinite_price\"])],\n",
    "                  axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_alpha[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"alpha_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_beta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"beta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of theta\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_theta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"theta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "exp_filename = moment + \"_K-{:d}_N-{:d}_M-{:d}_T-{:d}_randomseed-{:d}_Experiment.csv\".format(K, N, M, T, random_seed)\n",
    "data.to_csv(\".\\\\Result_Ferreira et.al. 2018 Algorithm 2\\\\\" + exp_filename, index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "source": [
    "# clear the memory\n",
    "H_reward = np.zeros(T) # the reward in each period\n",
    "H_bestX = np.zeros(shape = (T, K + 1)) # the best solution in each optimization\n",
    "H_P = np.zeros(shape = T) # the index of pricing vector we used in each period\n",
    "H_D = np.zeros(shape = (T, N)) # the demand of products in each period  \n",
    "H_I = np.zeros(shape = (T + 1, M)) # avaliable inventory in each period\n",
    "H_I[0, :] = np.float64(I_0)\n",
    "c = np.zeros(shape = (T, M))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "source": [
    "# if we replace step 1 with true theta\n",
    "for t in range(1, T+1):\n",
    "    # first step, sample from posterior distribution\n",
    "    # H_alpha[t-1, :, :], H_beta[t-1, :, :] is the history data from 0 to t\n",
    "    # H_theta[t-1, :, :] is the sample theta we used in round t\n",
    "#     H_theta[t-1, :, :] = mybeta(H_alpha[t-1, :, :], H_beta[t-1, :, :])\n",
    "\n",
    "    # first step, calculate the mean demand given sample theta\n",
    "    demand_mean = theta\n",
    "\n",
    "    # second step, optimize a linear function\n",
    "    model = pyscipopt.Model(\"Optimization in Rount {:d}\".format(t))\n",
    "    # generate decision variable\n",
    "    x = {}\n",
    "    for xindex in range(1, K+1):\n",
    "        x[xindex] = model.addVar(vtype=\"C\", lb = 0, ub = 1, name=\"x{:d}\".format(xindex))\n",
    "\n",
    "    # second step, generate object function\n",
    "    obj_coefficient = np.sum(demand_mean *  P_list, axis = 1)# obj_coefficient[k] = $\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1}$\n",
    "    model.setObjective(quicksum(x[xindex]*obj_coefficient[xindex-1] for xindex in range(1, K+1)), \"maximize\")\n",
    "    # objective = $\\sum_{k=1}^K(\\sum_{i=1}^N d_{i,k+1}(t)p_{i,k+1})x_{k}$\n",
    "\n",
    "    # second step, add constraint x_1+...+x_k<=1\n",
    "    model.addCons(quicksum(x[xindex] for xindex in range(1, K+1)) <= 1)\n",
    "\n",
    "    # second step, for each resources, we require \\sum_{k=1}^K\\sum_{i=1}^N d_{i,k}a_{i,j}x_l<=c_j\n",
    "    c[t-1, :] = H_I[t-1, :] / (T - t + 1)\n",
    "    for jj in range(1, M+1):\n",
    "        # size(A) = [N, M], size(demand_mean) = [K, N]\n",
    "        con_coefficient = A[:, jj - 1].dot(np.transpose(demand_mean))# con_coefficient[k] = $\\sum_{i=1}^N a_{i,j}d_{i,k+1}$\n",
    "        model.addCons(quicksum(x[xindex] * con_coefficient[xindex - 1] for xindex in range(1, K+1)) <= c[t - 1, jj - 1])\n",
    "\n",
    "    # second step, optimize the problem\n",
    "    model.optimize()\n",
    "    bestx = np.zeros(K+1) # p_{K+1} would force the demand be zero\n",
    "    for xindex in range(1,K+1):\n",
    "        bestx[xindex - 1] = model.getVal(x[xindex])\n",
    "    bestx[K] = 1 - np.sum(bestx[0:K])\n",
    "    eliminate_error = lambda x: 0 if np.abs(x) < 1e-10 else x #there would be numerical error in the best solution\n",
    "    bestx = np.array([eliminate_error(x) for x in bestx])\n",
    "    bestx = bestx / np.sum(bestx)\n",
    "\n",
    "    # third step, offer price\n",
    "    price_offered_index = np.random.choice(np.arange(1, K + 2), p = bestx)\n",
    "\n",
    "    # fourth step, update estimate of parameter\n",
    "    H_P[t - 1] = price_offered_index # record the index of offered price\n",
    "\n",
    "    # fourth step, record the optimal solution in this round\n",
    "    H_bestX[t - 1, :] = bestx\n",
    "\n",
    "    # fourth step, record the realization of demand\n",
    "    if price_offered_index < K + 1:\n",
    "        H_D[t - 1, :] = mybinomial(np.ones(N), theta[price_offered_index - 1, :]) # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_D[t - 1, :] = np.zeros(N)\n",
    "\n",
    "    # fourth step, record the reward in this period\n",
    "    if price_offered_index < K + 1:\n",
    "        H_reward[t - 1] = P_list[price_offered_index - 1, :].dot(H_D[t - 1, :])  # record the realization of demand\n",
    "    else: # the demand must be zero\n",
    "        H_reward[t - 1] = 0\n",
    "\n",
    "    # fourth step, record the remain inventory, size(A) = [N, M]\n",
    "    H_I[t, :] = H_I[t - 1, :] - np.transpose(A).dot(H_D[t - 1, :])\n",
    "    if not all(H_I[t, :] >= 0):\n",
    "        break\n",
    "\n",
    "print(\"Total reward of known distribution: {:f}\".format(np.sum(H_reward)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "source": [
    "# then input the experiment into csc file, we use period as index\n",
    "data = pd.DataFrame({\"Pricing_index\": H_P, \"Reward\": H_reward})\n",
    "data['Period_index'] = range(1, T+1)\n",
    "# add the demand of each product\n",
    "data = pd.concat([data, pd.DataFrame(H_D, columns = [\"product_{:d}_demand\".format(nindex) for nindex in range(1,N+1)])], axis = 1) \n",
    "# add the remain invetory of each resource\n",
    "data = pd.concat([data, pd.DataFrame(H_I[0:T, :], columns = [\"resouce_{:d}\".format(mindex) for mindex in range(1,M+1)])], axis = 1) \n",
    "# add the best solution in each round\n",
    "data = pd.concat([data, \n",
    "                  pd.DataFrame(H_bestX,\\\n",
    "                              columns = [\"use_price_index_{:d}\".format(kindex) for kindex in range(1,K+1)] + [\"use_infinite_price\"])],\n",
    "                  axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_alpha[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"alpha_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of alpha\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_beta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"beta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "# add the estimation of theta\n",
    "data = pd.concat([data,\\\n",
    "                 pd.DataFrame(np.reshape(a = H_theta[0:T, :, :], newshape =(T, K * N)),\n",
    "                             columns = [\"theta_k-{:d}_n-{:d}\".format(k, n) for k in range(1, K+1) for n in range(1, N+1) ])],\n",
    "                axis = 1)\n",
    "exp_filename = moment + \"_K-{:d}_N-{:d}_M-{:d}_T-{:d}_randomseed-{:d}_RealThetaExperiment.csv\".format(K, N, M, T, random_seed)\n",
    "data.to_csv(\".\\\\Result_Ferreira et.al. 2018 Algorithm 2\\\\\" + exp_filename, index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 3rd algotithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4 th alogrithm\n",
    "We implement the 4th algorithm of Ferreira et.al. 2018\n",
    "\n",
    "<img src=\"./Figure/Algorithem_4_of_Ferreira_et_al_2018.png\" style=\"zoom:70%\" />\n",
    "\n",
    "Following is its notation and description\n",
    "\n",
    "|      Parameter       | description                                                  |\n",
    "| :------------------: | :----------------------------------------------------------- |\n",
    "|         $K$          | Total number of available price vectors                      |\n",
    "|         $N$          | Total number of products                                    |\n",
    "|         $M$          | Total number of kinds of resource                            |\n",
    "|         $T$          | Total number of periods                                      |\n",
    "|        $[x]$         | We define $[x]$ as a set, $[x]=\\{1,2,\\cdots, x\\}$            |\n",
    "|         $i$          | Index of products                                            |\n",
    "|         $j$          | Index of resources                                           |\n",
    "|         $t$          | Index of period                                              |\n",
    "|     $I_j,I_j(t)$     | $I_j$ is the initial inventory for each resource $j\\in [M]$, $I_j(t)$ is the inventory at the end of period $t$. $I_j(0)=I_j$ |\n",
    "|       $a_{ij}$       | When we produced one unit item $i$, it would consume $a_{ij}$ unit $j$ |\n",
    "|        $c_j$         | we define $c_j=\\frac{I_j}{t}$                                |\n",
    "|        $p_k$         | We define $\\{p_1,p_2,\\cdots,p_K\\}$ as the admissible price vectors, each $p_k$ is a $N\\times 1$ vector, specifying the price of each product, $p_k=(p_{1k},\\cdots,p_{Nk})$, where $p_{ik}$ is the price of product $i$, for $i\\in [N]$. We define $p_{\\infty}$ as a \"shut-off\" price, such that the demand for any product under this price is zero. |\n",
    "|        $P(t)$        | We denote by $P(t)=(P_1(t),\\cdots,P_N(t))$ the prices chosen by the retailer in this period, and require that $P(t)\\in \\{p_1,p_2,\\cdots,p_K,p_{\\infty}\\}$ |\n",
    "|        $D(t)$        | We denote by $D(t) = (D_1(t),\\cdots,D_N(t))$ the demand of each product at period $t$. We assume that given $P(t)=p_k$, the demand $D(t)$ is sampled from a probability distribution on $\\mathbb{R}^{N}_+$ with joint cumulative distribution function (CDF) $F (x_1,\\cdots,x_N, pk, \\theta )$. $D(t)$ is independent of the history $\\mathcal{H}_{t-1}$, given $P(t)$ |\n",
    "|       $\\theta$       | $\\theta$ is the parameter of demand distribution, takes values in the parameter space $\\Theta\\subset\\mathbb{R}^l$. The nature would sample $\\theta$ from a prior distribution at the beginning form the process. The distribution is assumed to be subexponential; |\n",
    "|  $\\mathcal{H}_{t }$  | $\\mathcal{H}_{t}=(P(1),D(1),\\cdots,P(t),D(t))$               |\n",
    "|       $\\xi(t)$       | At the beginning of each period $t\\in[T]$, the retailer observes some context $\\xi(t)$, $\\xi(t)$ belongs to some discrete set $\\mathcal{X}$. We assume $\\xi(t)$ is sampled i.i.d from a known distribution. |\n",
    " | $d_{ik}(\\xi|\\theta)$ | The mean demand of product $i\\in [N]$ under price vector $p_k$, $\\forall k\\in[K]$, given context $\\xi$ and parameter $\\theta$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We assume the distribution of demand is calculated as follows: \n",
    "\\begin{equation}\n",
    "(D_1,D_2,\\cdots,D_{N}) \\sim F(d_1, d_2,\\cdots,d_N|P,\\xi,\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "$P$ is the price vector we offer, $\\xi$ is the observed context variable, $\\theta$ is the parameter that we need to estimate, it is sampled from the nature at the beginning of our study. We assume $\\theta \\sim f_{\\Theta}(\\theta)$\n",
    "\n",
    "We furtherly assume the density function of this distribution is $f(d_1, d_2,\\cdots,d_N|P,\\xi,\\theta)$.  Then we will derive the posterior distribution of $ \\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&f(\\theta \\ | \\ (P_1,\\xi_1,D^{(1)}), \\ (P_2,\\xi_2,D^{(2)}),\\cdots, \\ (P_t,\\xi_t,D^{(t)}))\\\\\n",
    "=&\\frac{f(\\theta, \\ (P_1,\\xi_1,D^{(1)}), \\ (P_2,\\xi_2,D^{(2)}),\\cdots, \\ (P_t,\\xi_t,D^{(t)}))}{\\int_{-\\infty}^{\\infty}f_{\\Theta}(\\theta)\\prod_{i=1}^tf(d^{(i)}_1, d^{(i)}_2,\\cdots,d^{(i)}_N|P_i,\\xi_i,\\theta)d\\theta}\\\\\n",
    "=&\\frac{f(\\ (P_1,\\xi_1,D^{(1)}), \\ (P_2,\\xi_2,D^{(2)}),\\cdots, \\ (P_t,\\xi_t,D^{(t)})\\ | \\ \\theta) f_{\\Theta}(\\theta)}{\\int_{-\\infty}^{\\infty}f_{\\Theta}(\\theta)\\prod_{i=1}^tf(d^{(i)}_1, d^{(i)}_2,\\cdots,d^{(i)}_N|P_i,\\xi_i,\\theta)d\\theta}\\\\\n",
    "=&\\frac{ f_{\\Theta}(\\theta)\\prod_{i=1}^t f(P_t,\\xi_t,D^{(t)}| \\ \\theta)}{\\int_{-\\infty}^{\\infty}f_{\\Theta}(\\theta)\\prod_{i=1}^tf(d^{(i)}_1, d^{(i)}_2,\\cdots,d^{(i)}_N|P_i,\\xi_i,\\theta)d\\theta}\\\\\n",
    "=&\\frac{ f_{\\Theta}(\\theta)\\prod_{i=1}^t f(D^{(i)}| \\ \\theta, P_t,\\xi_t,)}{\\int_{-\\infty}^{\\infty}f_{\\Theta}(\\theta)\\prod_{i=1}^tf(d^{(i)}_1, d^{(i)}_2,\\cdots,d^{(i)}_N|P_i,\\xi_i,\\theta)d\\theta}\\\\\n",
    "=&\\frac{ f_{\\Theta}(\\theta)\\prod_{i=1}^t f(d^{(i)}_1, d^{(i)}_2,\\cdots,d^{(i)}_N| \\ \\theta, P_t,\\xi_t,)}{\\int_{-\\infty}^{\\infty}f_{\\Theta}(\\theta)\\prod_{i=1}^tf(d^{(i)}_1, d^{(i)}_2,\\cdots,d^{(i)}_N|P_i,\\xi_i,\\theta)d\\theta}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$P_i$ is the pricing vector we choose in the $i^{th}$ period\n",
    "\n",
    "$\\xi_i$ is the context we observed in the $i^{th}$ period\n",
    "\n",
    "$D^{(i)}$ is the demand vector in $i^{th}$ period\n",
    "\n",
    "$d^{(i)}_j$ is the demand of $j^{th}$ product in $i^{th}$ period\n",
    "\n",
    "To implement this procedure, we need to solve 3 problems\n",
    "1. Given the real demand generator, we need to calculate its density function.\n",
    "2. sample random number from a customized density function\n",
    "3. calculate the integral, numerically\n",
    "\n",
    "We assume the demand follows discrete distribution, that is $d_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate parameters\n",
    "np.random.seed(12345)\n",
    "\n",
    "K = np.random.randint(low = 1, high = 5) # Total number of available price vectors\n",
    "N = np.random.randint(low = 1, high = 5) # Total number of products\n",
    "M = np.random.randint(low = 1, high = 5) # Total number of kinds of resource\n",
    "T = 1000 # Total number of periods\n",
    "\n",
    "# generate the demand \n",
    "\n",
    "# each row represent an admissible pricing strategy\n",
    "P_list = np.float64(np.random.randint(low = 1, high = 10, size = (K, M)))\n",
    "\n",
    "# type of context\n",
    "xi_type_num = np.random.randint(low = 3, high = 5 ) # maximum number of context type\n",
    "xi_list = np.float64(np.arange(1, xi_type_num, 1))\n",
    "\n",
    "# initialize inventory\n",
    "I_0 = np.random.randint(low = 9000, high = 11000, size = M)\n",
    "\n",
    "# initialize a_ij\n",
    "A = np.float64(np.random.randint(low = 5, high = 15, size = (N,M)))\n",
    "\n",
    "# real distribution of theta, we adopt beta distribution here, \n",
    "# here theta is the parameter of beta dirstribution\n",
    "# we assume the alpha and beta is independent of pricing vector , observed vector, and product\n",
    "alpha_real_scalar = np.random.randint(low = 1, high = 3)\n",
    "beta_real_scalar = np.random.randint(low = 2, high = 5)\n",
    "alpha_real = alpha_real_scalar * np.ones(shape = (K, xi_type, N))\n",
    "beta_real = beta_real_scalar * np.ones(shape = (K, xi_type, N))\n",
    "# alpha_real = np.random.randint(low = 1, high = 3, size = (K, xi_type, N))\n",
    "# beta_real = np.random.randint(low = 2, high = 5, size = (K, xi_type, N))\n",
    "\n",
    "# initialize history\n",
    "H_P = np.zeros(shape = T) # the index of pricing vector we used in each period\n",
    "H_xi = np.zeros(shape = T)# the index of observed context in each period\n",
    "H_D = np.zeros(shape = (T, N)) # the demand of products in each period  \n",
    "\n",
    "H_I = np.zeros(shape = (T, M)) # avaliable inventory in each period\n",
    "H_I[0, :] = np.float64(I_0)\n",
    "\n",
    "# each realization of price vector, observed context, index of period,\n",
    "# corresponds to a estimate\n",
    "H_alpha = np.zeros(shape = (T,K,xi_type_num,N))\n",
    "H_beta = np.zeros(shape = (T,K,xi_type_num,N))\n",
    "for kk in range(0, K):\n",
    "    for xi_index in range(0, xi_type_num):\n",
    "        H_alpha[0, kk, xi_index, :] = 1*np.ones(N)\n",
    "        H_beta[0, kk, xi_index, :] = 1*np.ones(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Given the pricing vector and parameter, we use this function to generate demand\n",
    "def GetDemand(P, xi, theta, num = 1):\n",
    "    # P is the pricing vector, N*1\n",
    "    # xi is the context observed by the merchant\n",
    "    # theta is the parameter of beta distribution, a N*2 vector\n",
    "    # num is the size of return demand, we would return M*num array, each column denote a sample demand\n",
    "    \n",
    "    # return value is a M*num array, each column denote a sample demand\n",
    "    demand_sample = np.zeros(N, num)\n",
    "    for ii in range(N):\n",
    "        demand_sample[ii, :] = np.random.beta(alpha = alpha[ii], beta = beta[ii], size = num) + xi*5 - P[ii]*2\n",
    "    return demand_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We derive the update formular here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def UpdatePosteriorDistribution(t, H_P, H_xi, H_D, H_alpha, H_beta):\n",
    "    # t is the index of period, the history is available from 0-t\n",
    "    # H_P is the index of pricing vector in each period\n",
    "    # H_xi is the index of obeserved context in each period\n",
    "    \n",
    "    # alpha_real and beta_real is the estimation of alpha and beta, given \n",
    "    alpha_real = alpha_real_scalar * np.ones(shape = (K, xi_type, N))\n",
    "    beta_real = beta_real_scalar * np.ones(shape = (K, xi_type, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyscipopt' has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0e6b062a2a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# implementation of algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyscipopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyscipopt' has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "# implementation of algorithm\n",
    "for tt in range(0, T):\n",
    "    # step 1, sample a randome parameter theta\n",
    "    alpha_tt = H_alpha[tt]\n",
    "    beta_tt = H_beta[tt]\n",
    "    # step 1, calculate the mean demand\n",
    "    d_mean_tt = np.zeros(N, K)\n",
    "    for kk in range(0, K): # traverse all the admissable pricing vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-92a6b828a7ec>:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.float(12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float64(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.33333333])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 5],\n",
       "       [6, 3],\n",
       "       [2, 7]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 8, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_list[:,0] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10339,  9654])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10339.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "       [ 9654.,     0.,     0., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function zeros in module numpy:\n",
      "\n",
      "zeros(...)\n",
      "    zeros(shape, dtype=float, order='C', *, like=None)\n",
      "    \n",
      "    Return a new array of given shape and type, filled with zeros.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    shape : int or tuple of ints\n",
      "        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "    dtype : data-type, optional\n",
      "        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "        `numpy.float64`.\n",
      "    order : {'C', 'F'}, optional, default: 'C'\n",
      "        Whether to store multi-dimensional data in row-major\n",
      "        (C-style) or column-major (Fortran-style) order in\n",
      "        memory.\n",
      "    like : array_like\n",
      "        Reference object to allow the creation of arrays which are not\n",
      "        NumPy arrays. If an array-like passed in as ``like`` supports\n",
      "        the ``__array_function__`` protocol, the result will be defined\n",
      "        by it. In this case, it ensures the creation of an array object\n",
      "        compatible with that passed in via this argument.\n",
      "    \n",
      "        .. note::\n",
      "            The ``like`` keyword is an experimental feature pending on\n",
      "            acceptance of :ref:`NEP 35 <NEP35>`.\n",
      "    \n",
      "        .. versionadded:: 1.20.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        Array of zeros with the given shape, dtype, and order.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    zeros_like : Return an array of zeros with shape and type of input.\n",
      "    empty : Return a new uninitialized array.\n",
      "    ones : Return a new array setting values to one.\n",
      "    full : Return a new array of given shape filled with value.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.zeros(5)\n",
      "    array([ 0.,  0.,  0.,  0.,  0.])\n",
      "    \n",
      "    >>> np.zeros((5,), dtype=int)\n",
      "    array([0, 0, 0, 0, 0])\n",
      "    \n",
      "    >>> np.zeros((2, 1))\n",
      "    array([[ 0.],\n",
      "           [ 0.]])\n",
      "    \n",
      "    >>> s = (2,2)\n",
      "    >>> np.zeros(s)\n",
      "    array([[ 0.,  0.],\n",
      "           [ 0.,  0.]])\n",
      "    \n",
      "    >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n",
      "    array([(0, 0), (0, 0)],\n",
      "          dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Contextual Bandits with UCB-based Exploration\n",
    "Notation:\n",
    "\n",
    "| Notation                 | Description                                                  |\n",
    "| :----------------------- | :----------------------------------------------------------- |\n",
    "| $K$                      | number of arms                                               |\n",
    "| $T$                      | number of total rounds                                       |\n",
    "| $t$                      | index of round                                               |\n",
    "| $x_{t,a}$                | $x_{t,a}\\in\\mathbb{R}^d$, $a\\in [K]$, it is the context, the context consists of $K$ feature vectors $\\{x_{t,a}\\in\\mathbb{R}^d|a\\in[K]\\}$ |\n",
    "| $a_t$                    | after observes the context, the agent select an action $a_t$ in round t |\n",
    "| $r_{t,a_t}$              | the reward after the agent select action $a_t$               |\n",
    "| $h$                      | we assume that $r_{t,a_t}=h(x_{t,a_t})+\\xi_t$, h is an unknown function satisfying $0\\le h(x)\\le 1$ for any x |\n",
    "| $\\xi_t$                  | $\\xi_t$ is v-sub-Gaussian noise conditioned on $x_{1,a_1},\\cdots,x_{t-1,a_{t-1}}$, satisfying $\\mathbb{E}\\xi_t=0$ |\n",
    "| $L$                      | the depth of neural network                                  |\n",
    "| $m$                      | number of neural in each layer of network                    |\n",
    "| $\\sigma(x)$              | we define $\\sigma(x)=\\max\\{x,0\\}$                            |\n",
    "| $W_1,\\cdots,W_{L-1},W_L$ | the weight in neural network. $W_1\\in\\mathbb{R}^{m\\times d}$, $W_i\\in\\mathbb{R}^{m\\times m}$, $2\\le i\\le L-1$, $W_L\\in\\mathbb{R}^{m\\times 1}$ |\n",
    "| $\\theta$                 | $\\theta=[vec(W_1)^T,\\cdots,vec(W_l)^T]\\in\\mathbb{R}^p$, $p=m+md+m^2(L-1)$ |\n",
    "| $f(x;\\theta)$            | we define $f(x;\\theta)=\\sqrt{m}W_L\\sigma(W_{l-1}\\sigma(\\cdots\\sigma(W_1x)))$ |\n",
    "\n",
    "\n",
    "<img src=\"./Figure/NeuralUCB_Initialization.png\" style=\"zoom:80%\" />\n",
    "\n",
    "Initialization of parameters:\n",
    "\n",
    "<img src=\"./Figure/NeuralUCB_Initialization.png\" style=\"zoom:80%\" />\n",
    "\n",
    "<img src=\"./Figure/NeuralUCB_Initialization2.png\" style=\"zoom:80%\" />\n",
    "\n",
    "UCB algorithm:\n",
    "\n",
    "<img src=\"./Figure/NeuralUCB_Algorithm1.png\" style=\"zoom:80%\" />\n",
    "\n",
    "<img src=\"./Figure/NeuralUCB_Algorithm2.png\" style=\"zoom:80%\" />\n",
    "\n",
    "we set $\\mathcal{L}(\\theta) = \\sum_{i=1}^t\\frac{(f(x_{i,a_i};\\theta)-r_{i,a_i})^2}{2}+\\frac{m\\lambda||\\theta-\\theta^{(0)}||^2_2}{2}$\n",
    "\n",
    "Then the gradient would be\n",
    "$$\n",
    "\\nabla\\mathcal{L}(\\theta) = \\sum_{i=1}^t(f(x_{i,a_i};\\theta)-r_{i,a_i})\\nabla f(x_{i,a_i};\\theta) + m\\lambda(\\theta-\\theta^{(0)})\n",
    "$$\n",
    "\n",
    "Forawar Algorithm of Neural Network\n",
    "$$\n",
    "\\begin{align}\n",
    "X_0 &= X\\\\\n",
    "X_1 &=\\sigma(W_1X_0)\\\\\n",
    "X_2 &=\\sigma(W_2X_1)\\\\\n",
    "\\cdots\\\\\n",
    "X_{L-1}&=\\sigma(W_{L-1}X_{L-2})\\\\\n",
    "X_{L} &=W_L X_{L-1}\n",
    "\\end{align}\n",
    "$$\n",
    "$f(X) = X_L$\n",
    "\n",
    "Backward Propagation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{X_L}f &= 1\\\\\n",
    "\\nabla_{W_L}f &= X_{L-1}\\\\\n",
    "\\nabla_{X_{L-1}}f &= W_{L}\\\\\n",
    "\\\\\n",
    "\\nabla_{W_{L-1}}f &= \\nabla_{W_{L-1}}f(X_{L-1}(W_{L-1}, X_{L-2}), W_L)=\\nabla_{X_{L-1}}f \\cdot \\nabla_{W_{L-1}}X_{L-1}(W_{L-1}, X_{L-2})\\\\\n",
    "\\nabla_{X_{L-2}}f &= \\nabla_{X_{L-2}}f(X_{L-1}(W_{L-1}, X_{L-2}), W_L)=\\nabla_{X_{L-1}}f \\cdot \\nabla_{X_{L-2}}X_{L-1}(W_{L-1}, X_{L-2})\\\\\n",
    "\\\\\n",
    "\\nabla_{W_{L-2}}f &= \\nabla_{W_{L-2}}f(X_{L-2}(W_{L-2}, X_{L-3}), W_L,W_{L-1})=\\nabla_{X_{L-2}}f \\cdot \\nabla_{W_{L-2}}X_{L-2}(W_{L-2}, X_{L-3})\\\\\n",
    "\\nabla_{X_{L-3}}f &= \\nabla_{X_{L-3}}f(X_{L-2}(W_{L-2}, X_{L-3}), W_L,W_{L-1})=\\nabla_{X_{L-2}}f \\cdot \\nabla_{X_{L-3}}X_{L-2}(W_{L-2}, X_{L-3})\\\\\n",
    "\\cdots\\\\\n",
    "\\nabla_{W_{l}}f &= \\nabla_{W_{l}}f(X_{l}(W_{l}, X_{l-1}), W_L,W_{L-1},\\cdots,W_{l+1})=\\nabla_{X_{l}}f \\cdot \\nabla_{W_{l}}X_{l}(W_{l}, X_{l-1})\\\\\n",
    "\\nabla_{X_{l-1}}f &= \\nabla_{X_{l-1}}f(X_{l}(W_{l}, X_{l-1}), W_L,W_{L-1},\\cdots,W_{l+1})=\\nabla_{X_{l}}f \\cdot \\nabla_{X_{l-1}}X_{l}(W_{l}, X_{l-1})\\\\\n",
    "\\cdots\\\\\n",
    "\\nabla_{W_{1}}f &= \\nabla_{W_{1}}f(X_{1}(W_{1}, X_{0}), W_L,W_{L-1},\\cdots,W_{2})=\\nabla_{X_{1}}f \\cdot \\nabla_{W_{1}}X_{1}(W_{1}, X_{0})\\\\\n",
    "\\nabla_{X_{0}}f &= \\nabla_{X_{0}}f(X_{1}(W_{1}, X_{0}), W_L,W_{L-1},\\cdots,W_{2})=\\nabla_{X_{1}}f \\cdot \\nabla_{X_{0}}X_{1}(W_{1}, X_{0})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\nabla_{W_{l}}f$ is a matrix, to be specific, $\\nabla_{W_{l}}f=\\left[\\begin{matrix}\\frac{\\partial f}{\\partial w^{(l)}_{11}}&\\cdots &\\frac{\\partial f}{\\partial w^{(l)}_{1m}}\\\\ \\vdots&& \\vdots\\\\ \\frac{\\partial f}{\\partial w^{(l)}_{m1}}&\\cdots &\\frac{\\partial f}{\\partial w^{(l)}_{mm}}\\end{matrix}\\right]$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial w^{(l)}_{11}}&\\cdots &\\frac{\\partial f}{\\partial w^{(l)}_{1m}}\\\\ \\vdots&& \\vdots\\\\ \\frac{\\partial f}{\\partial w^{(l)}_{m1}}&\\cdots &\\frac{\\partial f}{\\partial w^{(l)}_{mm}}\\end{matrix}\\right]=&\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l)}_{1}} \\frac{\\partial x^{(l)}_{1}}{\\partial w^{(l)}_{11}}&\\cdots &\\frac{\\partial f}{\\partial x^{(l)}_{1}}\\frac{\\partial x^{(l)}_{1}}{\\partial w^{(l)}_{1m}}\\\\ \\vdots&& \\vdots\\\\ \\frac{\\partial f}{\\partial x^{(l)}_{m}}\\frac{\\partial x^{(l)}_{m}}{\\partial w^{(l)}_{m1}}&\\cdots &\\frac{\\partial f}{\\partial x^{(l)}_{m}}\\frac{\\partial x^{(l)}_{m}}{\\partial w^{(l)}_{mm}}\\end{matrix}\\right]\\\\\n",
    "=&\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l)}_{1}} \\mathbb{1}_{\\sigma(w^{(l)}_{11}x^{(l-1)}_1+w^{(l)}_{12}x^{(l-1)}_2+\\cdots+w^{(l)}_{1m}x^{(l-1)}_m)>0} x^{(l-1)}_{1}&\\cdots &\\frac{\\partial f}{\\partial x^{(l)}_{1}}\\mathbb{1}_{\\sigma(w^{(l)}_{11}x^{(l-1)}_1+w^{(l)}_{12}x^{(l-1)}_2+\\cdots+w^{(l)}_{1m}x^{(l-1)}_m)>0} x^{(l-1)}_{m}\\\\ \\vdots&& \\vdots\\\\ \\frac{\\partial f}{\\partial x^{(l)}_{m}}\\mathbb{1}_{\\sigma(w^{(l)}_{m1}x^{(l-1)}_1+w^{(l)}_{m2}x^{(l-1)}_2+\\cdots+w^{(l)}_{mm}x^{(l-1)}_m)>0} x^{(l-1)}_{1}&\\cdots &\\frac{\\partial f}{\\partial x^{(l)}_{m}}\\mathbb{1}_{\\sigma(w^{(l)}_{m1}x^{(l-1)}_1+w^{(l)}_{m2}x^{(l-1)}_2+\\cdots+w^{(l)}_{mm}x^{(l-1)}_m)>0} x^{(l-1)}_{m}\\end{matrix}\\right]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\nabla_{X_{l-1}}f$ is a vector, to be specific, $\\nabla_{X_{l-1}}f=\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l-1)}_{1}}\\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial x^{(l-1)}_{m}}\\end{matrix}\\right]$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l-1)}_{1}}\\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial x^{(l-1)}_{m}}\\end{matrix}\\right]=&\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l)}_{1}}\\frac{\\partial x^{(l)}_1}{\\partial x^{(l-1)}_1}+\\frac{\\partial f}{\\partial x^{(l)}_{2}}\\frac{\\partial x^{(l)}_{2}}{\\partial x^{(l-1)}_{1}}+\\cdots+\\frac{\\partial f}{\\partial x^{(l)}_{m}}\\frac{\\partial x^{(l)}_{m}}{\\partial x^{(l-1)}_{1}}\\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial x^{(l)}_{1}}\\frac{\\partial x^{(l)}_{1}}{\\partial x^{(l-1)}_{m}}+\\frac{\\partial f}{\\partial x^{(l)}_{2}}\\frac{\\partial x^{(l)}_{2}}{\\partial x^{(l-1)}_{m}}+\\cdots+\\frac{\\partial f}{\\partial x^{(l)}_{m}}\\frac{\\partial x^{(l)}_{m}}{\\partial x^{(l-1)}_{m}}\\end{matrix}\\right]\\\\\n",
    "=&\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l)}_{1}}\\mathbb{1}_{\\sigma(w^{(l)}_{11}x^{(l-1)}_1+w^{(l)}_{12}x^{(l-1)}_2+\\cdots+w^{(l)}_{1m}x^{(l-1)}_m)>0}w^{(l)}_{11}+\\frac{\\partial f}{\\partial x^{(l)}_{2}}\\mathbb{1}_{\\sigma(w^{(l)}_{21}x^{(l-1)}_1+w^{(l)}_{22}x^{(l-1)}_2+\\cdots+w^{(l)}_{2m}x^{(l-1)}_m)>0}w^{(l)}_{21}+\\cdots+\\frac{\\partial f}{\\partial x^{(l)}_{m}}\\mathbb{1}_{\\sigma(w^{(l)}_{m1}x^{(l-1)}_1+w^{(l)}_{m2}x^{(l-1)}_2+\\cdots+w^{(l)}_{mm}x^{(l-1)}_m)>0}w^{(l)}_{m1}\\\\ \n",
    "\\vdots\\\\ \n",
    "\\frac{\\partial f}{\\partial x^{(l)}_{1}}\\mathbb{1}_{\\sigma(w^{(l)}_{11}x^{(l-1)}_1+w^{(l)}_{12}x^{(l-1)}_2+\\cdots+w^{(l)}_{1m}x^{(l-1)}_m)>0}w^{(l)}_{1m}+\\frac{\\partial f}{\\partial x^{(l)}_{2}}\\mathbb{1}_{\\sigma(w^{(l)}_{21}x^{(l-1)}_1+w^{(l)}_{22}x^{(l-1)}_2+\\cdots+w^{(l)}_{2m}x^{(l-1)}_m)>0}w^{(l)}_{2m}+\\cdots+\\frac{\\partial f}{\\partial x^{(l)}_{m}}\\mathbb{1}_{\\sigma(w^{(l)}_{m1}x^{(l-1)}_1+w^{(l)}_{m2}x^{(l-1)}_2+\\cdots+w^{(l)}_{mm}x^{(l-1)}_m)>0}w^{(l)}_{mm}\\end{matrix}\\right ]\\\\\n",
    "=&\\left[\\begin{matrix}w^{(l)}_{11}&\\cdots& w^{(l)}_{1m}\\\\ \\vdots&&\\vdots\\\\ w^{(l)}_{m1}&\\cdots& w^{(l)}_{mm}\\end{matrix}\\right]^T\n",
    "\\left[\\begin{matrix}\\frac{\\partial f}{\\partial x^{(l)}_{1}}\\mathbb{1}_{\\sigma(w^{(l)}_{11}x^{(l-1)}_1+w^{(l)}_{12}x^{(l-1)}_2+\\cdots+w^{(l)}_{1m}x^{(l-1)}_m)>0}\\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial x^{(l)}_{m}}\\mathbb{1}_{\\sigma(w^{(l)}_{m1}x^{(l-1)}_1+w^{(l)}_{m2}x^{(l-1)}_2+\\cdots+w^{(l)}_{mm}x^{(l-1)}_m)>0}\\end{matrix}\\right]\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# the setting is based on the description of section 7.1 of the papaer\n",
    "\n",
    "# Set the parameter of the network\n",
    "L = 2\n",
    "m = 20 \n",
    "\n",
    "# Set the parameter of the game\n",
    "K = 2# Total number of actions, \n",
    "T = 100 # Total number of periods\n",
    "d = 2 # the dimension of context\n",
    "\n",
    "# Total number of parameters in the nerual network\n",
    "# the paper claims that p = m+md+m^2(L-1)\n",
    "# let L = 2, \n",
    "# it also claim that f(x,\\theta)=\\sqrt{m}W_2\\sigma(W_1x)\n",
    "# then the total number of neurals should be m + m * d + m * m * (L - 2) \n",
    "p = m + m * d + m * m * (L - 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We assume the reward follows reward = context^T * A^T * A * context + \\xi\n",
    " \n",
    " $\\xi$ is a random variable following standard normal distribution N(0, 1)\n",
    " \n",
    " A is d\\*d matrix, randomly generated from N(0, 1)\n",
    " \n",
    " We assume the context is independent from the action and round index. \n",
    " \n",
    " Given action a and round index t, the context is randomly sample from a unit ball in dimension d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the hyper parameter of neural network here\n",
    "# we fix gamma in each round, according to the description of section 3.1\n",
    "gamma_t = 0.01 #{0.01, 0.1, 1, 10}\n",
    "v = 0.1 #{0.01, 0.1, 1}\n",
    "lambda_ = 0.01 #{0.01, 0.1, 1}\n",
    "delta = 0.01 #{0.01, 0.1, 1}\n",
    "S = 0.01 #{0.01, 0.1, 1, 10}\n",
    "eta = 1e-3 #{0.001, 0.01, 0.1}\n",
    "# we set J equal to round index t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# following are functions that are used to play the game \n",
    "def SampleContext(d, K):\n",
    "    # according to the description, the context is uniformly distributed on a d-dimension sphere\n",
    "    # d is the dimension of context, a scalar\n",
    "    # K is the total number of arts, a scalar\n",
    "    \n",
    "    # this function return context, as an d*K matrix, each column corresponds a context of action\n",
    "    \n",
    "    context = np.random.normal(loc=0, scale=1, size=(d, K))\n",
    "    length = np.sqrt( np.sum(context * context, axis = 0) )\n",
    "    length = np.tile(length, (d, 1))\n",
    "    context = context / length # each column represent a context\n",
    "    return context\n",
    "\n",
    "def GetRealReward(context, A):\n",
    "    # context is the context of arm, a d*1 vector\n",
    "    # A is the d*d matrix,\n",
    "    \n",
    "    # this function return the reward\n",
    "    \n",
    "    return context.transpose().dot(A.transpose().dot(A)).dot(context) + np.random.normal(loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     1,
     7,
     17,
     34
    ]
   },
   "outputs": [],
   "source": [
    "# following are functions that are related to the neural network\n",
    "def Relu(x):\n",
    "    # Relu function\n",
    "    # if x is a scalar, the return value would be a scalar\n",
    "    # if x is a matrix, the return value would be a matrix\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def ReluDerivative(x):\n",
    "    # the derivative of Relu function\n",
    "    # if x is a scalar, the return value would be a scalar\n",
    "    # if x is a matrix, the return value would be a matrix\n",
    "#     return np.array([(lambda x: 1 if x > 0 else 0)(xi) for xi in x])\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def NeuralNetwork(X, params, L, m):\n",
    "    # X is the input, each column correspont to a context\n",
    "    # params is a dictionay, each key corresponds to the weight in each layer\n",
    "    # its keys are \"w1\" , \"w2\", ..., \"w{:d}\".format(L).\n",
    "    # L is the number of layers\n",
    "    # m is the number of neurals in each layer\n",
    "    \n",
    "    # the return value would be a dictionary, its key would be like \"l0\", \"l1\", \"l2\", ..., \"l{:d}\".format(L), \n",
    "    # each key represent the value of a layer, \"l0\" is X, the last layer is output\n",
    "    \n",
    "    X_layer = {}\n",
    "    X_layer[\"x0\"] = X\n",
    "    for l in range(1, L):\n",
    "        X_layer[\"x\" + str(l)] = Relu(params[\"w\" + str(l)].dot(X_layer[\"x\" + str(l-1)]))\n",
    "    X_layer[\"x\" + str(L)] = np.sqrt(m) * params[\"w\" + str(L)].dot(X_layer[\"x\" + str(L-1)])\n",
    "    return X_layer\n",
    "\n",
    "def GradientNeuralNetwork(X, params, L, m):\n",
    "    # this function calculate the gradient of each parameter in the neural network\n",
    "    # X is the context, a 1 dimension vecotr in R^d\n",
    "    # params is a dictionary that stores the paraemeters of neural network\n",
    "    # its keys are \"w1\" , \"w2\", ..., \"w{:d}\".format(L).\n",
    "    # L is the number of layers\n",
    "    # m is the number of neurals in each layer\n",
    "    \n",
    "    # vectorize the function we used\n",
    "    myRelu = np.vectorize(Relu)\n",
    "    myReluDerivative = np.vectorize(ReluDerivative)\n",
    "    \n",
    "    # we firstly calculate the value of each layer\n",
    "    X_layer = NeuralNetwork(X, params, L, m)\n",
    "    \n",
    "    # then we calculate the gradient of \"X_layer\" and gradient of parameter\n",
    "    grad_X_layer = {}\n",
    "    grad_parameter = {}\n",
    "    \n",
    "    grad_X_layer[\"x\" + str(L)] = 1\n",
    "    grad_parameter[\"w\" + str(L)] = np.sqrt(m) * X_layer[\"x\" + str(L - 1)]\n",
    "    grad_X_layer[\"x\" + str(L - 1)] = np.sqrt(m) * params[\"w\" + str(L)][0, :]\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        grad_parameter[\"w\" + str(l) ] = grad_X_layer[\"x\" + str(l)] * myReluDerivative(X_layer[\"x\" + str(l)])\n",
    "        grad_parameter[\"w\" + str(l) ] = np.matmul(np.expand_dims(grad_parameter[\"w\" + str(l) ], axis=1),\\\n",
    "                                                  np.expand_dims(X_layer[\"x\" + str(l - 1)], axis = 0))\n",
    "\n",
    "        grad_X_layer[\"x\" + str(l - 1)] = grad_X_layer[\"x\" + str(l)] * myReluDerivative(X_layer[\"x\" + str(l)])\n",
    "        grad_X_layer[\"x\" + str(l - 1)] = np.matmul(params[\"w\" + str(l)].transpose(),\\\n",
    "                                                   np.expand_dims(grad_X_layer[\"x\" + str(l - 1)] , axis = 1))\n",
    "    return grad_parameter\n",
    "\n",
    "def FlattenGradient(grad_parameter, L):\n",
    "    # accroding to the paper, the function f should be a vector in R^p\n",
    "    # but what we got in GradientNeuralNetwork is a dictionary,\n",
    "    # we would use this function to convert the dictinary into a vector\n",
    "    # grad_parameter is the gradient stored in a dictionary\n",
    "    # L is the total number of layers\n",
    "    \n",
    "    # the return value is the flattern vector = [vec(w1)^T, vec(w2)^T, ..., vec(w_L)^T]^T\n",
    "    \n",
    "    # we firstly generate the order of all the parameter\n",
    "    para_order = [\"w\" + str(l) for l in range(1, L + 1)] #e.g. para_order = [\"w1\", \"w2\", \"w3\"]\n",
    "    # then we can combine all the 1 d arrays together\n",
    "    gradient = np.concatenate([grad_parameter[para_name].flatten() for para_name in para_order])\n",
    "    return gradient\n",
    "    \n",
    "def GradientLossFunction(X, params, L, m, r, theta_0, lambda_):\n",
    "    # this function calculate the gradient of lossfunction\n",
    "    # X is the matrix of observed contexts, each column represent a context\n",
    "    # X is required to be a 2-D matrix here, even sometimes it may only contain one column\n",
    "    # params is a dictionary that stores the paraemeters of neural network\n",
    "    # its keys are \"w1\" , \"w2\", ..., \"w{:d}\".format(L).\n",
    "    # L is the number of layers\n",
    "    # m is the number of neurals in each layer\n",
    "    # r is the reward in each round\n",
    "    # r is required to be a vector here, even sometimes it may only contain one column\n",
    "    # theta_0 is the initalized value of parameters, restored as a disctionary\n",
    "    \n",
    "    #we would repeatedly call GradientNeuralNetwork() to calculate the gradients here\n",
    "    \n",
    "    # firstly, we calculate the shape of X and r\n",
    "    context_num = len(r)\n",
    "    \n",
    "    # secondly, we calculate the value of each layer\n",
    "    X_layer = NeuralNetwork(X, params, L, m) # each value in X_layer would be a \n",
    "    \n",
    "    # secondly, we repeatedly call GradientNeuralNetwork() to calculate the gradient of regression part\n",
    "    grad_loss = {}# apply for space\n",
    "    for key in params.keys():\n",
    "        grad_loss[key] = np.zeros(params[key].shape)\n",
    "    \n",
    "    for ii in range(1, context_num + 1):\n",
    "        new_term = GradientNeuralNetwork(X[:, ii - 1], params, L, m)\n",
    "        for key in grad_loss.keys():\n",
    "            grad_loss[key] = grad_loss[key] + new_term[key] * (X_layer[\"x\" + str(L)][0, ii - 1] - r[ii - 1])\n",
    "#             grad_loss[key] = grad_loss[key] + new_term[key] * (X_layer[\"x\" + str(L)][0, ii - 1] - r[ii - 1]) / context_num\n",
    "    \n",
    "    # thirdly, we calculate the gradient of regularization\n",
    "    for key in grad_loss.keys():\n",
    "        grad_loss[key] = grad_loss[key] + m * lambda_ * (params[key] - theta_0[key])\n",
    "    \n",
    "    return grad_loss\n",
    "\n",
    "# X = np.array([[1, 1], [2, 3]])\n",
    "# lambda_ = 0\n",
    "# theta_0 = {\"w1\": np.zeros((4, 2)),\n",
    "#            \"w2\": np.zeros((1,4))}\n",
    "# r = np.array([0, 0])\n",
    "# L = 2\n",
    "# m = 4\n",
    "# params = {\"w1\": np.array([[1, 2], [-3, 2], [0, -1], [3, 3]]),\n",
    "#           \"w2\": np.array([[1, 2, 3, -1]])}\n",
    "# grad_loss = GradientLossFunction(X, params, L, m, r, theta_0, lambda_)\n",
    "# X_layer = NeuralNetwork(X, params, L, m)\n",
    "# print(X_layer)\n",
    "# print(grad_loss)\n",
    "\n",
    "# grad_parameter = GradientNeuralNetwork(X[:, 0], params, L, m)\n",
    "# print(grad_parameter)\n",
    "# grad_parameter = GradientNeuralNetwork(X[:, 1], params, L, m)\n",
    "# print(grad_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1, predicted reward -0.000424,          predicted upper bound 0.090640,          actual reward2.272125\n",
      "round 2, predicted reward 0.068763,          predicted upper bound 0.114806,          actual reward1.644206\n",
      "round 3, predicted reward 0.035864,          predicted upper bound 0.112615,          actual reward2.374053\n",
      "round 4, predicted reward 0.133829,          predicted upper bound 0.159915,          actual reward0.621021\n",
      "round 5, predicted reward 0.125655,          predicted upper bound 0.175896,          actual reward0.225242\n",
      "round 6, predicted reward 0.160420,          predicted upper bound 0.172339,          actual reward1.690004\n",
      "round 7, predicted reward 0.777125,          predicted upper bound 0.800167,          actual reward-0.560492\n",
      "round 8, predicted reward 0.983655,          predicted upper bound 1.010498,          actual reward0.744462\n",
      "round 9, predicted reward 1.065241,          predicted upper bound 1.076667,          actual reward-0.820915\n",
      "round 10, predicted reward 0.283184,          predicted upper bound 0.310808,          actual reward1.749863\n",
      "round 11, predicted reward 0.703293,          predicted upper bound 0.738148,          actual reward1.670348\n",
      "round 12, predicted reward 1.105121,          predicted upper bound 1.141263,          actual reward0.433601\n",
      "round 13, predicted reward 1.212571,          predicted upper bound 1.232599,          actual reward1.332337\n",
      "round 14, predicted reward 0.474813,          predicted upper bound 0.492311,          actual reward-0.914758\n",
      "round 15, predicted reward 1.054616,          predicted upper bound 1.076929,          actual reward2.096148\n",
      "round 16, predicted reward 0.316751,          predicted upper bound 0.331554,          actual reward0.559662\n",
      "round 17, predicted reward 1.622827,          predicted upper bound 1.635132,          actual reward1.440866\n",
      "round 18, predicted reward 0.930713,          predicted upper bound 0.973603,          actual reward1.502516\n",
      "round 19, predicted reward 1.353896,          predicted upper bound 1.373078,          actual reward0.416201\n",
      "round 20, predicted reward 1.559543,          predicted upper bound 1.568139,          actual reward0.512476\n",
      "round 21, predicted reward 0.834536,          predicted upper bound 0.867873,          actual reward0.425917\n",
      "round 22, predicted reward 1.317566,          predicted upper bound 1.328963,          actual reward2.076005\n",
      "round 23, predicted reward 1.364796,          predicted upper bound 1.393930,          actual reward-0.160265\n",
      "round 24, predicted reward 1.417134,          predicted upper bound 1.426571,          actual reward-1.330353\n",
      "round 25, predicted reward 1.056596,          predicted upper bound 1.067803,          actual reward0.817492\n",
      "round 26, predicted reward 0.295162,          predicted upper bound 0.315495,          actual reward0.331979\n",
      "round 27, predicted reward 1.039549,          predicted upper bound 1.048849,          actual reward1.552421\n",
      "round 28, predicted reward 1.288948,          predicted upper bound 1.322777,          actual reward1.542083\n",
      "round 29, predicted reward 1.139483,          predicted upper bound 1.150642,          actual reward1.281281\n",
      "round 30, predicted reward 0.928884,          predicted upper bound 0.943935,          actual reward1.528348\n",
      "round 31, predicted reward 0.526687,          predicted upper bound 0.542968,          actual reward1.059209\n",
      "round 32, predicted reward 1.145122,          predicted upper bound 1.155893,          actual reward0.838127\n",
      "round 33, predicted reward 1.031941,          predicted upper bound 1.057640,          actual reward-0.269146\n",
      "round 34, predicted reward 0.979327,          predicted upper bound 0.996686,          actual reward1.154079\n",
      "round 35, predicted reward 1.324289,          predicted upper bound 1.338024,          actual reward-0.529172\n",
      "round 36, predicted reward 1.217855,          predicted upper bound 1.227487,          actual reward0.535593\n",
      "round 37, predicted reward 0.759149,          predicted upper bound 0.770074,          actual reward1.832194\n",
      "round 38, predicted reward 1.029710,          predicted upper bound 1.040862,          actual reward0.805005\n",
      "round 39, predicted reward 1.251582,          predicted upper bound 1.260370,          actual reward1.802072\n",
      "round 40, predicted reward 0.688033,          predicted upper bound 0.701778,          actual reward1.214210\n",
      "round 41, predicted reward 1.014587,          predicted upper bound 1.024450,          actual reward-0.445522\n",
      "round 42, predicted reward 0.791284,          predicted upper bound 0.803736,          actual reward0.657000\n",
      "round 43, predicted reward 1.055941,          predicted upper bound 1.065663,          actual reward0.939589\n",
      "round 44, predicted reward 0.729106,          predicted upper bound 0.742272,          actual reward0.658019\n",
      "round 45, predicted reward 0.383343,          predicted upper bound 0.403322,          actual reward0.924442\n",
      "round 46, predicted reward 1.073350,          predicted upper bound 1.082610,          actual reward1.287910\n",
      "round 47, predicted reward 1.051787,          predicted upper bound 1.060078,          actual reward0.485091\n",
      "round 48, predicted reward 1.265421,          predicted upper bound 1.279897,          actual reward1.047104\n",
      "round 49, predicted reward 1.084986,          predicted upper bound 1.091933,          actual reward2.123704\n",
      "round 50, predicted reward 0.876523,          predicted upper bound 0.886180,          actual reward-0.025674\n",
      "round 51, predicted reward 1.229329,          predicted upper bound 1.243771,          actual reward1.423720\n",
      "round 52, predicted reward 0.492564,          predicted upper bound 0.508513,          actual reward0.822949\n",
      "round 53, predicted reward 0.924992,          predicted upper bound 0.933414,          actual reward-0.581617\n",
      "round 54, predicted reward 1.301908,          predicted upper bound 1.311082,          actual reward1.544164\n",
      "round 55, predicted reward 0.890817,          predicted upper bound 0.899964,          actual reward1.086808\n",
      "round 56, predicted reward 0.752403,          predicted upper bound 0.764056,          actual reward0.326341\n",
      "round 57, predicted reward 1.402462,          predicted upper bound 1.411922,          actual reward1.975315\n",
      "round 58, predicted reward 0.964382,          predicted upper bound 0.974079,          actual reward1.409764\n",
      "round 59, predicted reward 0.690136,          predicted upper bound 0.698323,          actual reward0.360301\n",
      "round 60, predicted reward 1.255878,          predicted upper bound 1.270911,          actual reward1.927858\n",
      "round 61, predicted reward 1.412268,          predicted upper bound 1.421108,          actual reward1.417952\n",
      "round 62, predicted reward 0.931399,          predicted upper bound 0.941860,          actual reward0.604083\n",
      "round 63, predicted reward 1.266366,          predicted upper bound 1.276277,          actual reward1.276250\n",
      "round 64, predicted reward 0.919050,          predicted upper bound 0.948325,          actual reward1.783952\n",
      "round 65, predicted reward 1.418305,          predicted upper bound 1.425967,          actual reward0.098886\n",
      "round 66, predicted reward 1.158272,          predicted upper bound 1.166320,          actual reward0.272244\n",
      "round 67, predicted reward 0.690655,          predicted upper bound 0.711684,          actual reward-0.158905\n",
      "round 68, predicted reward 0.483797,          predicted upper bound 0.494244,          actual reward1.397000\n",
      "round 69, predicted reward 0.603170,          predicted upper bound 0.633071,          actual reward0.938655\n",
      "round 70, predicted reward 1.001356,          predicted upper bound 1.009557,          actual reward-0.160535\n",
      "round 71, predicted reward 1.402945,          predicted upper bound 1.426201,          actual reward2.354198\n",
      "round 72, predicted reward 0.474606,          predicted upper bound 0.485840,          actual reward2.270110\n",
      "round 73, predicted reward 0.624751,          predicted upper bound 0.635042,          actual reward0.276979\n",
      "round 74, predicted reward 1.284081,          predicted upper bound 1.294900,          actual reward0.914317\n",
      "round 75, predicted reward 1.527126,          predicted upper bound 1.536955,          actual reward1.243799\n",
      "round 76, predicted reward 1.427915,          predicted upper bound 1.441402,          actual reward3.202854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 77, predicted reward 0.665389,          predicted upper bound 0.685482,          actual reward2.574900\n",
      "round 78, predicted reward 1.112521,          predicted upper bound 1.121283,          actual reward1.642484\n",
      "round 79, predicted reward 0.397173,          predicted upper bound 0.406823,          actual reward0.404134\n",
      "round 80, predicted reward 1.215799,          predicted upper bound 1.239467,          actual reward0.852867\n",
      "round 81, predicted reward 0.714865,          predicted upper bound 0.724042,          actual reward2.350419\n",
      "round 82, predicted reward 0.387147,          predicted upper bound 0.403358,          actual reward0.107181\n",
      "round 83, predicted reward 1.337601,          predicted upper bound 1.345103,          actual reward-0.036781\n",
      "round 84, predicted reward 1.699848,          predicted upper bound 1.710142,          actual reward0.361713\n",
      "round 85, predicted reward 1.638043,          predicted upper bound 1.645842,          actual reward1.355636\n",
      "round 86, predicted reward 0.531219,          predicted upper bound 0.542413,          actual reward-0.570691\n",
      "round 87, predicted reward 0.526051,          predicted upper bound 0.543833,          actual reward0.040482\n",
      "round 88, predicted reward 2.052348,          predicted upper bound 2.060005,          actual reward3.109648\n",
      "round 89, predicted reward 1.538383,          predicted upper bound 1.544247,          actual reward0.032526\n",
      "round 90, predicted reward 1.647936,          predicted upper bound 1.653114,          actual reward1.561023\n",
      "round 91, predicted reward 1.463811,          predicted upper bound 1.468371,          actual reward0.730551\n",
      "round 92, predicted reward 0.569166,          predicted upper bound 0.585575,          actual reward-0.606856\n",
      "round 93, predicted reward 0.404037,          predicted upper bound 0.415614,          actual reward-0.218457\n",
      "round 94, predicted reward 0.937470,          predicted upper bound 0.949118,          actual reward1.422678\n",
      "round 95, predicted reward 1.599067,          predicted upper bound 1.605454,          actual reward0.572199\n",
      "round 96, predicted reward 1.302039,          predicted upper bound 1.314894,          actual reward2.442352\n",
      "round 97, predicted reward 1.315582,          predicted upper bound 1.325291,          actual reward1.104891\n",
      "round 98, predicted reward 1.823110,          predicted upper bound 1.831120,          actual reward0.521556\n",
      "round 99, predicted reward 0.701061,          predicted upper bound 0.721147,          actual reward1.124101\n",
      "round 100, predicted reward 0.889067,          predicted upper bound 0.901706,          actual reward1.802893\n"
     ]
    }
   ],
   "source": [
    "# we implement the algorithm here\n",
    "from copy import deepcopy\n",
    "\n",
    "#set the random seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "# initialize the value of parameter\n",
    "theta_0 = {}\n",
    "W = np.random.normal(loc = 0, scale = 4 / m, size=(int(m/2), int(m/2)))\n",
    "w = np.random.normal(loc = 0, scale = 4 / m, size=(1, int(m/2)))\n",
    "for key in range(1, L + 1):\n",
    "    if key == 1:\n",
    "        # this paper doesn't present the initialization of w1\n",
    "        # in its setting, d = m, then he let theta_0[\"w1\"]=[W,0;0,W]\n",
    "        # but in fact d might not equal to m\n",
    "        tempW = np.random.normal(loc = 0, scale = 4 / m, size=(int(m/2), int(d/2)))\n",
    "        theta_0[\"w1\"] = np.zeros((m, d))\n",
    "        theta_0[\"w1\"][0:int(m/2), 0:int(d/2)] = tempW\n",
    "        theta_0[\"w1\"][int(m/2):, int(d/2):] = tempW\n",
    "    elif 2 <= key and key <= L - 1:\n",
    "        theta_0[\"w\" + str(key)] = np.zeros((m, m))\n",
    "        theta_0[\"w\" + str(key)][0:int(m/2), 0:int(m/2)] = W\n",
    "        theta_0[\"w\" + str(key)][int(m/2):, 0:int(m/2):] = W\n",
    "    else:\n",
    "        theta_0[\"w\" + str(key)] = np.concatenate([w, -w], axis = 1)\n",
    "        \n",
    "params = deepcopy(theta_0)\n",
    "Z_t_minus1 = lambda_ * np.eye(p)\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# implement NeuralUCB\n",
    "reward = np.zeros(T)\n",
    "X_history = np.zeros((d, T))\n",
    "params_history = {}\n",
    "grad_history = {}\n",
    "for tt in range(1, T + 1):\n",
    "    # observe \\{x_{t,a}\\}_{a=1}^{k=1}\n",
    "    context_list = SampleContext(d, K)\n",
    "    \n",
    "    # compute the upper bound of reward\n",
    "    U_t_a = np.zeros(K) # the upper bound of K actions\n",
    "    predict_reward = np.zeros(K)\n",
    "    for a in range(1, K + 1):\n",
    "        predict_reward[a - 1] = NeuralNetwork(context_list[:, a - 1], params, L, m)['x' + str(L)][0]\n",
    "        grad_parameter = GradientNeuralNetwork(context_list[:, a - 1], params, L, m)\n",
    "        grad_parameter = FlattenGradient(grad_parameter, L)\n",
    "        Z_t_minus1_inverse = np.linalg.inv(Z_t_minus1)\n",
    "        U_t_a[a - 1] = predict_reward[a - 1] + gamma_t * np.sqrt(grad_parameter.dot(Z_t_minus1_inverse).dot(grad_parameter)/m)\n",
    "    ind = np.argmax(U_t_a, axis=None) # ind is the index of action we would play\n",
    "    \n",
    "    # play ind and observe reward\n",
    "    X_history[:, tt - 1] = context_list[:, ind]\n",
    "    reward[tt - 1] = GetRealReward(context_list[:, ind], A)\n",
    "    print(\"round {:d}, predicted reward {:4f},\\\n",
    "          predicted upper bound {:4f},\\\n",
    "          actual reward{:4f}\".format(tt, predict_reward[ind], U_t_a[ind], reward[tt - 1]))\n",
    "    assert(np.abs(predict_reward[ind]) <= 50)\n",
    "    \n",
    "    # compute Z_t_minus1\n",
    "    grad_parameter = GradientNeuralNetwork(context_list[:, ind], params, L, m)\n",
    "    grad_parameter = FlattenGradient(grad_parameter, L)\n",
    "    grad_parameter = np.expand_dims(grad_parameter, axis = 1)\n",
    "    Z_t_minus1 = Z_t_minus1 + grad_parameter.dot(grad_parameter.transpose()) / m\n",
    "    \n",
    "    # train neural network\n",
    "    J = tt\n",
    "    for j in range(J):\n",
    "        grad_loss = GradientLossFunction(X_history[:, 0:tt], params, L, m, reward[0:tt], theta_0, lambda_)\n",
    "        for key in params.keys():\n",
    "            params[key] = params[key] - eta * grad_loss[key]\n",
    "            \n",
    "    params_history[tt] = params\n",
    "    grad_history[tt] = grad_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.85659335284052\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "np.random.seed(12345)\n",
    "\n",
    "reward = np.zeros(T)\n",
    "X_history = np.zeros((d, T))\n",
    "params_history = {}\n",
    "grad_history = {}\n",
    "for tt in range(1, T + 1):\n",
    "    # observe \\{x_{t,a}\\}_{a=1}^{k=1}\n",
    "    context_list = SampleContext(d, K)\n",
    "    \n",
    "    # compute the upper bound of reward\n",
    "    predict_reward = np.zeros(K)\n",
    "    for a in range(1, K + 1):\n",
    "        predict_reward[a - 1] = context_list[:, a - 1].transpose().dot(A.transpose().dot(A)).dot(context_list[:, a - 1])\n",
    "          \n",
    "    ind = np.argmax(predict_reward, axis=None) # ind is the index of action we would play\n",
    "    \n",
    "    # play ind and observe reward\n",
    "    X_history[:, tt - 1] = context_list[:, ind]\n",
    "    reward[tt - 1] = GetRealReward(context_list[:, ind], A)\n",
    "    print(\"round {:d}, predicted reward {:4f},\\\n",
    "          actual reward{:4f}\".format(tt, predict_reward[ind], reward[tt - 1]))\n",
    "    assert(np.abs(predict_reward[ind]) <= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# code for reference\n",
    "import h5py\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    define scale function\n",
    "    \"\"\"\n",
    "    return np.exp(x)/(1.0+np.exp(x))\n",
    "\n",
    "def RELU(x):\n",
    "    return np.np.maximum(x,0)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    return np.array([reluDerivativeSingleElement(xi) for xi in x])\n",
    "\n",
    "def reluDerivativeSingleElement(xi):\n",
    "    if xi > 0:\n",
    "        return 1\n",
    "    elif xi <= 0:\n",
    "        return 0\n",
    "    \n",
    "def compute_loss(Y,V):\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(V)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "    return L\n",
    "    \n",
    "\n",
    "def feed_forward(X, params):\n",
    "    tempt={}\n",
    "    tempt[\"Z\"]=np.matmul(params[\"W\"], X) + params[\"b1\"]\n",
    "    tempt[\"H\"]=sigmoid(tempt[\"Z\"])\n",
    "    #tempt[\"H\"]=RELU(tempt[\"Z\"])\n",
    "    tempt[\"U\"]=np.matmul(params[\"C\"], tempt[\"H\"]) + params[\"b2\"]\n",
    "    tempt[\"V\"]=np.exp(tempt[\"U\"]) / np.sum(np.exp(tempt[\"U\"]), axis=0)\n",
    "    return tempt\n",
    "\n",
    "def back_propagate(X, Y, params, tempt, m_batch):\n",
    "    dU=tempt[\"V\"]-Y\n",
    "    dC=(1. / m_batch) * np.matmul(dU, tempt[\"H\"].T)\n",
    "    db2=(1. / m_batch) * np.sum(dU, axis=1, keepdims=True)\n",
    "    dH=np.matmul(params[\"C\"].T, dU)\n",
    "    dZ = dH * sigmoid(tempt[\"Z\"]) * (1 - sigmoid(tempt[\"Z\"]))\n",
    "    #dZ=dH*reluDerivative(tempt[\"Z\"])\n",
    "    dW = (1. / m_batch) * np.matmul(dZ, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    grads={\"dW\":dW, \"db1\":db1, \"dC\":dC, \"db2\":db2}\n",
    "    return grads\n",
    "\n",
    "#hyperparameters\n",
    "epochs=10\n",
    "batch_size=1\n",
    "batchs=np.int32(60000/batch_size)\n",
    "LR=0.01\n",
    "dh=100#number of hidden nodes\n",
    "\n",
    "#getting 60000 samples of training data and 10000 samples of testing data\n",
    "f=h5py.File('MNISTdata.hdf5','r')\n",
    "x_test_set=np.float32(f['x_test'][:])\n",
    "y_test_set=np.int32(np.array(f['y_test'][:,0])).reshape(-1,1)\n",
    "x_train_set=np.float32(f['x_train'][:])\n",
    "y_train_set=np.int32(np.array(f['y_train'][:,0])).reshape(-1,1)\n",
    "f.close()\n",
    "X=np.vstack((x_train_set,x_test_set))\n",
    "Y=np.vstack((y_train_set,y_test_set))\n",
    "num_samples=Y.shape[0]\n",
    "Y=Y.reshape(1,num_samples)\n",
    "Y_new = np.eye(10)[Y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(10, num_samples)\n",
    "X_train, X_test=X[:60000].T, X[60000:].T\n",
    "Y_train, Y_test=Y_new[:,:60000], Y_new[:,60000:]\n",
    "\n",
    "#building fully connected neural network with one hidden layer\n",
    "#initialization of parameters\n",
    "params={\"b1\":np.zeros((dh,1)),\n",
    "        \"W\":np.random.randn(dh,784)*np.sqrt(1. / 784),\n",
    "        \"b2\":np.zeros((10,1)),\n",
    "        \"C\":np.random.randn(10,dh)*np.sqrt(1. / dh)}\n",
    "\n",
    "\n",
    "#training the network\n",
    "for num_epoches in range(epochs):\n",
    "    if (num_epoches > 5):\n",
    "        LR = 0.001\n",
    "    if (num_epoches > 10):\n",
    "        LR = 0.0001\n",
    "    if (num_epoches > 15):\n",
    "        LR = 0.00001\n",
    "    #shuffle the training data\n",
    "    shuffle_index=np.random.permutation(X_train.shape[1])\n",
    "    X_train= X_train[:, shuffle_index]\n",
    "    Y_train=Y_train[:, shuffle_index]\n",
    "    \n",
    "    for num_batch in range(batchs):\n",
    "        left_index=num_batch*batch_size\n",
    "        right_index=min(left_index+batch_size,x_train_set.shape[0]-1)\n",
    "        m_batch=right_index-left_index\n",
    "        X=X_train[:,left_index:right_index]\n",
    "        Y=Y_train[:,left_index:right_index]\n",
    "\n",
    "        tempt=feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, tempt, 1)\n",
    "\n",
    "        #gradient descent\n",
    "        params[\"W\"] = params[\"W\"] - LR * grads[\"dW\"]\n",
    "        params[\"b1\"] = params[\"b1\"] - LR * grads[\"db1\"]\n",
    "        params[\"C\"] = params[\"C\"] - LR * grads[\"dC\"]\n",
    "        params[\"b2\"] = params[\"b2\"] - LR * grads[\"db2\"]\n",
    "    \n",
    "    #compute loss on training data\n",
    "    tempt = feed_forward(X_train, params)\n",
    "    train_loss = compute_loss(Y_train, tempt[\"V\"])\n",
    "    #compute loss on test set\n",
    "    tempt=feed_forward(X_test, params)\n",
    "    test_loss = compute_loss(Y_test, tempt[\"V\"])\n",
    "    total_correct=0\n",
    "    for n in range(Y_test.shape[1]):\n",
    "        p = tempt[\"V\"][:,n]\n",
    "        prediction = np.argmax(p)\n",
    "        if prediction == np.argmax(Y_test[:,n]):\n",
    "            total_correct+=1\n",
    "    accuracy = np.float32(total_correct) / (Y_test.shape[1])\n",
    "    #print(params)\n",
    "    print(\"Epoch {}: training loss = {}, test loss = {}, accuracy={}\".format(\n",
    "            num_epoches + 1, train_loss, test_loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
