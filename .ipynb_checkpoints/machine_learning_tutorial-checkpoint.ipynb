{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "### http://yann.lecun.com/exdb/mnist/\n",
    "### The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (60000, 784) float32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py #pip install h5py -- https://www.h5py.org/\n",
    "\n",
    "#load train\n",
    "f = h5py.File('MNISTdata.hdf5', 'r')\n",
    "train_x, train_y = f['x_train'][:], f['y_train'][:,0]\n",
    "f.close()\n",
    "\n",
    "print(\"train_x\", train_x.shape, train_x.dtype)\n",
    "#each image is stored in 784*1 numpy.ndarray, basically 28*28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x10f8f9310>, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0].reshape(28, 28)), train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):#Fully connected Neural Network\n",
    "    \"\"\"FNN.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"FNN Builder.\"\"\"\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(784, 100),#100 is the number of hidden nodes in the hidden layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "        #self.layer1 = nn.Linear(784, 100)\n",
    "        #self.layer2 = nn.ReLU(inplace=True)\n",
    "        #self.layer3 = nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "        \n",
    "        #x = self.layer1(x)\n",
    "        #x = self.layer2(x)\n",
    "        #x = self.layer3(x)\n",
    "        \n",
    "        #y = self.fc_layer(x)\n",
    "        #return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784*100 + 100*10 - NN\n",
    "# 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(loader, is_gpu):\n",
    "    \"\"\"Calculate accuracy.\n",
    "\n",
    "    Args:\n",
    "        loader (torch.utils.data.DataLoader): training / test set loader\n",
    "        is_gpu (bool): whether to run on GPU\n",
    "    Returns:\n",
    "        tuple: (overall accuracy, class level accuracy)\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        if is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)#forward\n",
    "        \n",
    "    \n",
    "        total += labels.size(0)\n",
    "        #correct += (predicted == labels).sum()\n",
    "        correct += (predicted == labels[:,0].T).sum()\n",
    "\n",
    "    return 100*correct.item()/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters settings\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--wd', type=float, default=5e-4, help='weight decay')#lr/(c+wd)\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size_train', type=int,\n",
    "                    default=16, help='training set input batch size')\n",
    "parser.add_argument('--batch_size_test', type=int,\n",
    "                    default=16, help='test set input batch size')\n",
    "parser.add_argument('--is_gpu', type=bool, default=False,\n",
    "                    help='whether training using GPU')\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "    \n",
    "# parse the arguments\n",
    "opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('MNISTdata.hdf5','r')\n",
    "\n",
    "x_test_set=np.float32(f['x_test'][:])\n",
    "y_test_set=np.int32(np.array(f['y_test'][:,0])).reshape(-1,1)\n",
    "x_train_set=np.float32(f['x_train'][:])\n",
    "y_train_set=np.int32(np.array(f['y_train'][:,0])).reshape(-1,1)\n",
    "\n",
    "f.close()\n",
    "\n",
    "#num_samples = y_train_set.shape[0]\n",
    "#y_train_set = y_train_set.reshape(1, num_samples)\n",
    "#y_train_set = np.eye(10)[y_train_set.astype('int32')]\n",
    "#y_train_set = y_train_set.T.reshape(10, num_samples)\n",
    "\n",
    "#num_samples = y_test_set.shape[0]\n",
    "#y_test_set = y_test_set.reshape(1, num_samples)\n",
    "#y_test_set = np.eye(10)[y_test_set.astype('int32')]\n",
    "#y_test_set = y_test_set.T.reshape(10, num_samples)\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train_set), torch.Tensor(y_train_set)) # create your datset\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "#mini-batch gradient, stochastic gradient descent - 1 sample\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(torch.Tensor(x_test_set), torch.Tensor(y_test_set)) # create your datset\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=opt.batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataset.TensorDataset,\n",
       " torch.utils.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset), type(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "# create the FNN instance\n",
    "net = FNN()\n",
    "# For training on GPU, transfer net and data into the GPU\n",
    "if opt.is_gpu:\n",
    "    net = net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print('Training on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()#N dim -> prob (softmax) -> CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=opt.lr, weight_decay=opt.wd)#a variant of SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7f954ea7ca0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/env_full/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/env_full/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #if training on GPU, wrap the data into the cuda\n",
    "        if opt.is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)#forward\n",
    "        loss = criterion(outputs, labels[:, 0].long())\n",
    "        loss.backward()#compute gradients\n",
    "        optimizer.step()#descent\n",
    "\n",
    "        # calculate loss\n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "    # Normalizing the loss by the total number of train batches\n",
    "    running_loss /= len(trainloader)\n",
    "\n",
    "    # Calculate training/test set accuracy of the existing model\n",
    "    train_accuracy = calculate_accuracy(trainloader, opt.is_gpu)\n",
    "    test_accuracy = calculate_accuracy(testloader, opt.is_gpu)\n",
    "\n",
    "    print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(\n",
    "        epoch+1, running_loss, train_accuracy, test_accuracy))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2028, grad_fn=<NllLossBackward>), True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,  loss.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.5531, -10.6170,  -5.4915,  -3.7804,   4.3626,  -2.2602,  -5.7757,\n",
       "          -0.8041,  -2.5927,   4.5236],\n",
       "        [-10.6401,  -5.0171,   0.8678,   1.1943,  -3.1983,  -4.5186,  -7.5350,\n",
       "           4.1555,  -1.7577,   1.6817],\n",
       "        [ -5.6284, -10.8048,  -1.3184,   1.7533,  -5.9923,  -2.7121, -11.7568,\n",
       "           8.9870,  -3.8250,   2.4643],\n",
       "        [ -2.9973,  -6.7424,  -7.2857,   1.5633,  -2.7529,   5.5822,  -5.4267,\n",
       "          -0.6405,  -1.5046,   2.2351],\n",
       "        [ 11.7629, -13.9371,   0.7229,  -3.8191,  -8.5875,   1.0126,  -0.7213,\n",
       "          -5.5325,   0.2129,  -2.4998],\n",
       "        [ -1.4576,  -5.5969,  -4.6671,  -1.7559,  -0.0714,   0.8205,  -5.1608,\n",
       "           4.8438,  -4.0908,   1.5565],\n",
       "        [ -1.2305,  -3.7770,   0.6684,  -4.0692,  -0.0636,  -2.6266,   5.9102,\n",
       "          -4.9157,  -2.4583,  -1.2941],\n",
       "        [  1.5573,  -3.8231,   4.6996,   4.3359, -14.4627,   0.1409,  -3.4533,\n",
       "          -4.9336,  -1.8440,  -6.5170],\n",
       "        [ -1.4865,  -8.8501,  -1.6032,  -3.0433,  -2.0102,  -1.1915,  -7.5230,\n",
       "           6.1737,  -1.4339,   2.5665],\n",
       "        [ -2.6364,  -3.1044,   6.8589,   1.2248,  -2.9760,  -1.2720,  -0.2092,\n",
       "          -5.8921,  -1.4205,  -5.6947],\n",
       "        [  0.3940,  -7.0767,   1.4884,  -3.5630,  -2.9641,  -0.8749,   8.7756,\n",
       "         -13.2933,   1.5566,  -4.5682],\n",
       "        [  0.3824,  -9.0988,   0.9962,  -3.6890,   3.7839,  -0.2791,   0.7645,\n",
       "          -2.5876,  -4.1056,  -2.0936],\n",
       "        [ -5.9300,  -8.7531,  -4.2214,  -0.7451,   0.7194,  -1.2686,  -4.8129,\n",
       "          -1.2015,  -0.8750,   4.1938],\n",
       "        [ -0.0314,  -9.3810,  10.0974,   0.1664,  -3.8410,  -0.9671,   2.7391,\n",
       "         -16.0111,  -1.1944,  -8.6991],\n",
       "        [ -6.1631,  -5.7467,  -2.0893,   8.5651,  -4.0058,  -0.3835, -10.6164,\n",
       "          -1.4438,  -1.7298,  -2.4850],\n",
       "        [ -2.6197,  -4.5164,  -0.0209,   5.9236,  -8.4071,   1.6177,  -6.4669,\n",
       "          -6.3101,   0.2442,  -3.8457]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 7, 7, 5, 0, 7, 6, 3, 7, 2, 6, 4, 9, 2, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, 0].long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.23334259795546275, test loss = 0.23107303718807204, accuracy=0.9319\n",
      "Epoch 2: training loss = 0.17363187679587486, test loss = 0.17460703908888603, accuracy=0.9449\n",
      "Epoch 3: training loss = 0.12975675211847817, test loss = 0.13803599922775564, accuracy=0.9597\n",
      "Epoch 4: training loss = 0.10486907887581996, test loss = 0.11517843963481662, accuracy=0.9643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-98bb1fc8fb85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m#gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    define scale function\n",
    "    \"\"\"\n",
    "    return np.exp(x)/(1.0+np.exp(x))\n",
    "\n",
    "def RELU(x):\n",
    "    return np.np.maximum(x,0)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    return np.array([reluDerivativeSingleElement(xi) for xi in x])\n",
    "\n",
    "def reluDerivativeSingleElement(xi):\n",
    "    if xi > 0:\n",
    "        return 1\n",
    "    elif xi <= 0:\n",
    "        return 0\n",
    "    \n",
    "def compute_loss(Y,V):\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(V)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "    return L\n",
    "    \n",
    "\n",
    "def feed_forward(X, params):\n",
    "    tempt={}\n",
    "    tempt[\"Z\"]=np.matmul(params[\"W\"], X) + params[\"b1\"]\n",
    "    tempt[\"H\"]=sigmoid(tempt[\"Z\"])\n",
    "    #tempt[\"H\"]=RELU(tempt[\"Z\"])\n",
    "    tempt[\"U\"]=np.matmul(params[\"C\"], tempt[\"H\"]) + params[\"b2\"]\n",
    "    tempt[\"V\"]=np.exp(tempt[\"U\"]) / np.sum(np.exp(tempt[\"U\"]), axis=0)\n",
    "    return tempt\n",
    "\n",
    "def back_propagate(X, Y, params, tempt, m_batch):\n",
    "    # X is m*n matrix\n",
    "    # Y is m*1 matrix\n",
    "    # tempt is the value in each neural cell\n",
    "    dU=tempt[\"V\"]-Y # the loss of output layer\n",
    "    dC=(1. / m_batch) * np.matmul(dU, tempt[\"H\"].T)\n",
    "    db2=(1. / m_batch) * np.sum(dU, axis=1, keepdims=True)\n",
    "    dH=np.matmul(params[\"C\"].T, dU)\n",
    "    dZ = dH * sigmoid(tempt[\"Z\"]) * (1 - sigmoid(tempt[\"Z\"]))\n",
    "    #dZ=dH*reluDerivative(tempt[\"Z\"])\n",
    "    dW = (1. / m_batch) * np.matmul(dZ, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    grads={\"dW\":dW, \"db1\":db1, \"dC\":dC, \"db2\":db2}\n",
    "    return grads\n",
    "\n",
    "#hyperparameters\n",
    "epochs=10\n",
    "batch_size=1\n",
    "batchs=np.int32(60000/batch_size)\n",
    "LR=0.01\n",
    "dh=100#number of hidden nodes\n",
    "\n",
    "#getting 60000 samples of training data and 10000 samples of testing data\n",
    "f=h5py.File('MNISTdata.hdf5','r')\n",
    "x_test_set=np.float32(f['x_test'][:])\n",
    "y_test_set=np.int32(np.array(f['y_test'][:,0])).reshape(-1,1)\n",
    "x_train_set=np.float32(f['x_train'][:])\n",
    "y_train_set=np.int32(np.array(f['y_train'][:,0])).reshape(-1,1)\n",
    "f.close()\n",
    "X=np.vstack((x_train_set,x_test_set))\n",
    "Y=np.vstack((y_train_set,y_test_set))\n",
    "num_samples=Y.shape[0]\n",
    "Y=Y.reshape(1,num_samples)\n",
    "Y_new = np.eye(10)[Y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(10, num_samples)\n",
    "X_train, X_test=X[:60000].T, X[60000:].T\n",
    "Y_train, Y_test=Y_new[:,:60000], Y_new[:,60000:]\n",
    "\n",
    "#building fully connected neural network with one hidden layer\n",
    "#initialization of parameters\n",
    "params={\"b1\":np.zeros((dh,1)),\n",
    "        \"W\":np.random.randn(dh,784)*np.sqrt(1. / 784),\n",
    "        \"b2\":np.zeros((10,1)),\n",
    "        \"C\":np.random.randn(10,dh)*np.sqrt(1. / dh)}\n",
    "\n",
    "\n",
    "#training the network\n",
    "for num_epoches in range(epochs):\n",
    "    if (num_epoches > 5):\n",
    "        LR = 0.001\n",
    "    if (num_epoches > 10):\n",
    "        LR = 0.0001\n",
    "    if (num_epoches > 15):\n",
    "        LR = 0.00001\n",
    "    #shuffle the training data\n",
    "    shuffle_index=np.random.permutation(X_train.shape[1])\n",
    "    X_train= X_train[:, shuffle_index]\n",
    "    Y_train=Y_train[:, shuffle_index]\n",
    "    \n",
    "    for num_batch in range(batchs):\n",
    "        left_index=num_batch*batch_size\n",
    "        right_index=min(left_index+batch_size,x_train_set.shape[0]-1)\n",
    "        m_batch=right_index-left_index\n",
    "        X=X_train[:,left_index:right_index]\n",
    "        Y=Y_train[:,left_index:right_index]\n",
    "\n",
    "        tempt=feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, tempt, 1)\n",
    "\n",
    "        #gradient descent\n",
    "        params[\"W\"] = params[\"W\"] - LR * grads[\"dW\"]\n",
    "        params[\"b1\"] = params[\"b1\"] - LR * grads[\"db1\"]\n",
    "        params[\"C\"] = params[\"C\"] - LR * grads[\"dC\"]\n",
    "        params[\"b2\"] = params[\"b2\"] - LR * grads[\"db2\"]\n",
    "    \n",
    "    #compute loss on training data\n",
    "    tempt = feed_forward(X_train, params)\n",
    "    train_loss = compute_loss(Y_train, tempt[\"V\"])\n",
    "    #compute loss on test set\n",
    "    tempt=feed_forward(X_test, params)\n",
    "    test_loss = compute_loss(Y_test, tempt[\"V\"])\n",
    "    total_correct=0\n",
    "    for n in range(Y_test.shape[1]):\n",
    "        p = tempt[\"V\"][:,n]\n",
    "        prediction = np.argmax(p)\n",
    "        if prediction == np.argmax(Y_test[:,n]):\n",
    "            total_correct+=1\n",
    "    accuracy = np.float32(total_correct) / (Y_test.shape[1])\n",
    "    #print(params)\n",
    "    print(\"Epoch {}: training loss = {}, test loss = {}, accuracy={}\".format(\n",
    "            num_epoches + 1, train_loss, test_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model with JD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read/write data from/to local files\n",
    "prefix_path = 'JD_data/'\n",
    "# 'skus' table\n",
    "skus = pd.read_csv(prefix_path + 'JD_sku_data.csv')\n",
    "# 'users' table\n",
    "users = pd.read_csv(prefix_path + 'JD_user_data.csv')\n",
    "# 'clicks' table\n",
    "clicks = pd.read_csv(prefix_path + 'JD_click_data.csv')\n",
    "# 'orders' table\n",
    "orders = pd.read_csv(prefix_path + 'JD_order_data.csv')\n",
    "# 'delivery' table\n",
    "delivery = pd.read_csv(prefix_path + 'JD_delivery_data.csv')\n",
    "# 'inventory' table\n",
    "inventory = pd.read_csv(prefix_path + 'JD_inventory_data.csv')\n",
    "# 'network' table\n",
    "network = pd.read_csv(prefix_path + 'JD_network_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['order_date'] = pd.to_datetime(orders['order_date']) \n",
    "orders['weekday'] = orders['order_date'].dt.dayofweek\n",
    "df_temp = orders[['weekday','final_unit_price']]\n",
    "#Add dummy variables\n",
    "df_temp1 = pd.get_dummies(df_temp['weekday'], prefix='weekday')\n",
    "cols_to_keep = ['final_unit_price']\n",
    "df_temp = df_temp[cols_to_keep].join(df_temp1.iloc[:,0:])#not df_temp1.ix[:,0:], consider the gender case\n",
    "df_temp['intercept'] = 1\n",
    "train_cols_ = df_temp.columns[1:]#can write ['x1', 'x2'] manually\n",
    "train_df = df_temp[train_cols_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt2 = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_JD = torch.utils.data.TensorDataset(torch.Tensor(train_df.values), torch.Tensor(df_temp['final_unit_price'].values)) # create your datset\n",
    "trainloader_JD = torch.utils.data.DataLoader(\n",
    "    trainset_JD, batch_size=opt2.batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_JD(nn.Module):\n",
    "    \"\"\"FNN.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"FNN Builder.\"\"\"\n",
    "        super(FNN_JD, self).__init__()\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4, 1)\n",
    "        )\n",
    "        #self.fc_layer = nn.Sequential(\n",
    "        #    nn.Linear(8, 4),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #    nn.Linear(4, 2),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #    nn.Linear(2, 1)\n",
    "        #)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "# create the FNN instance\n",
    "net_JD = FNN_JD()\n",
    "# For training on GPU, transfer net and data into the GPU\n",
    "if opt2.is_gpu:\n",
    "    net_JD = net.cuda()\n",
    "    net_JD = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print('Training on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion_JD = nn.MSELoss()\n",
    "optimizer_JD = optim.Adam(net_JD.parameters(), lr=opt2.lr, weight_decay=opt2.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549984</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549985</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549988</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549989 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        weekday_0  weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  \\\n",
       "0               0          0          0          1          0          0   \n",
       "1               0          0          0          1          0          0   \n",
       "2               0          0          0          1          0          0   \n",
       "3               0          0          0          1          0          0   \n",
       "4               0          0          0          1          0          0   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "549984          0          0          0          0          0          1   \n",
       "549985          0          0          0          0          0          1   \n",
       "549986          0          0          0          0          0          1   \n",
       "549987          0          0          0          0          0          1   \n",
       "549988          0          0          0          0          0          1   \n",
       "\n",
       "        weekday_6  intercept  \n",
       "0               0          1  \n",
       "1               0          1  \n",
       "2               0          1  \n",
       "3               0          1  \n",
       "4               0          1  \n",
       "...           ...        ...  \n",
       "549984          0          1  \n",
       "549985          0          1  \n",
       "549986          0          1  \n",
       "549987          0          1  \n",
       "549988          0          1  \n",
       "\n",
       "[549989 rows x 8 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(opt2.epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader_JD, 0):\n",
    "        # get the inputs\n",
    "        inputs, prices = data\n",
    "        \n",
    "        #if training on GPU, wrap the data into the cuda\n",
    "        if opt2.is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            prices = prices.cuda()\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, prices = Variable(inputs), Variable(prices)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_JD.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net_JD(inputs)\n",
    "        loss = criterion_JD(outputs[:,0], prices)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        optimizer_JD.step()\n",
    "\n",
    "        # calculate loss\n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "    # Normalizing the loss by the total number of train batches\n",
    "    #running_loss /= len(trainloader)\n",
    "\n",
    "    # Calculate training/test set accuracy of the existing model\n",
    "    #train_accuracy = calculate_accuracy(trainloader, opt.is_gpu)\n",
    "\n",
    "    print(\"Iteration: {0} | Loss: {1}\".format(\n",
    "        epoch+1, running_loss))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3165746048"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum of squared error\n",
    "opt2.batch_size_train * 197859128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to improve accuracy:\n",
    "### 1. hyperparameter tuning: different algorithm and learning rate - SGD, different loss function, batch size\n",
    "### 2. different network structures, different activiation layer\n",
    "### 3. more features/inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       final_unit_price   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     37.65\n",
      "Date:                Sat, 22 Aug 2020   Prob (F-statistic):           5.94e-46\n",
      "Time:                        07:32:01   Log-Likelihood:            -3.1613e+06\n",
      "No. Observations:              549989   AIC:                         6.323e+06\n",
      "Df Residuals:                  549982   BIC:                         6.323e+06\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "weekday_1      1.7547      0.416      4.218      0.000       0.939       2.570\n",
      "weekday_2      0.7403      0.405      1.829      0.067      -0.053       1.534\n",
      "weekday_3     -1.2450      0.372     -3.344      0.001      -1.975      -0.515\n",
      "weekday_4     -1.2111      0.395     -3.063      0.002      -1.986      -0.436\n",
      "weekday_5      3.0500      0.403      7.563      0.000       2.260       3.840\n",
      "weekday_6      1.5055      0.429      3.511      0.000       0.665       2.346\n",
      "intercept     71.1017      0.298    238.376      0.000      70.517      71.686\n",
      "==============================================================================\n",
      "Omnibus:                  1500889.055   Durbin-Watson:                   1.923\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):     406614359100.909\n",
      "Skew:                          33.204   Prob(JB):                         0.00\n",
      "Kurtosis:                    4214.783   Cond. No.                         8.65\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "df_temp = orders[['weekday','final_unit_price']]\n",
    "#Add dummy variables\n",
    "df_temp1 = pd.get_dummies(df_temp['weekday'], prefix='weekday')\n",
    "cols_to_keep = ['final_unit_price']\n",
    "df_temp = df_temp[cols_to_keep].join(df_temp1.iloc[:,1:])#not df_temp1.ix[:,0:], consider the gender case\n",
    "df_temp['intercept'] = 1\n",
    "train_cols_ = df_temp.columns[1:]#can write ['x1', 'x2'] manually\n",
    "train_df = df_temp[train_cols_]\n",
    "linear_model = sm.OLS(df_temp['final_unit_price'], train_df)\n",
    "res = linear_model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weekday_1     1.754674\n",
       "weekday_2     0.740332\n",
       "weekday_3    -1.245017\n",
       "weekday_4    -1.211051\n",
       "weekday_5     3.050009\n",
       "weekday_6     1.505508\n",
       "intercept    71.101683\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = res.params.values\n",
    "x = train_df.values\n",
    "y = df_temp['final_unit_price']\n",
    "loss = 0\n",
    "for i in range(len(y)):\n",
    "    predict = np.dot(coef, x[i])\n",
    "    loss += (predict - y[i])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3165328109.540477"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8*4 + 4*1\n",
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
